{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e5f6b6-0505-4698-a4ad-d4de2a08070c",
   "metadata": {},
   "source": [
    "# Multi-Strategy Document Retrieval and Evaluation System\n",
    "\n",
    "## Objective\n",
    "Build a Python/LangChain system that compares four different document retrieval strategies against a golden source benchmark, evaluating their accuracy and cost-effectiveness for question-answering tasks.\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "### 1. Document Dataset (Excel)\n",
    "**File:** `document_dataset.xlsx`\n",
    "| TOC_Number | Text |\n",
    "|------------|------|\n",
    "| 1.1 | Introduction to machine learning fundamentals |\n",
    "| 1.2 | Supervised learning algorithms and applications |\n",
    "| ... | ... |\n",
    "\n",
    "### 2. Golden Source Questions (Excel)\n",
    "**File:** `golden_source.xlsx`\n",
    "| Question_ID | Question_Text | TOC_1 | TOC_2 | TOC_3 | TOC_4 | TOC_5 |\n",
    "|-------------|--------------|--------|--------|--------|--------|--------|\n",
    "| Q1 | How do I evaluate my ML model? | 2.2 | 1.2 | 2.1 | 1.3 | 3.1 |\n",
    "| Q2 | What are neural network types? | 3.1 | 3.2 | 1.1 | 1.2 | 2.1 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| Q10 | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "### 3. Category Definitions\n",
    "- **Category_1** (Topic): List of valid categories to be provided\n",
    "- **Category_2** (Level): List of valid categories to be provided\n",
    "\n",
    "## System Components\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "#### 1.1 Document Categorization\n",
    "- For each row in the document dataset:\n",
    "  - Automatically assign Category_1 (Topic) based on text content\n",
    "  - Automatically assign Category_2 (Level) based on text complexity\n",
    "- Methods:\n",
    "  - Zero-shot classification using LLM\n",
    "  - Or embedding similarity to category descriptions\n",
    "  - Or keyword/rule-based assignment\n",
    "\n",
    "#### 1.2 Processed Dataset Structure\n",
    "After categorization, create:\n",
    "| TOC_Number | Text | Category_1 | Category_2 |\n",
    "|------------|------|------------|------------|\n",
    "| 1.1 | Introduction to machine learning fundamentals | Fundamentals | Beginner |\n",
    "| 1.2 | Supervised learning algorithms and applications | Algorithms | Intermediate |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "#### 1.3 Embedding Generation\n",
    "- Create embeddings for all document texts (OpenAI/Claude)\n",
    "- Store embeddings with metadata\n",
    "\n",
    "### 2. Question Processing\n",
    "- Embed each incoming question\n",
    "- Categorize questions into both category types using same method as documents\n",
    "\n",
    "### 3. Four Retrieval Strategies\n",
    "\n",
    "#### Strategy A: Pure Embedding Similarity\n",
    "- Compute cosine similarity between question and document embeddings\n",
    "- Return top 5 most similar documents\n",
    "- Cost: Embedding lookup only\n",
    "\n",
    "#### Strategy B: Category Filtering\n",
    "- Filter documents matching question's categories\n",
    "- Return up to 5 matches (by TOC order)\n",
    "- Cost: Metadata query only\n",
    "\n",
    "#### Strategy C: Hybrid (Categories + Similarity)\n",
    "- Filter by categories first\n",
    "- Rank filtered results by embedding similarity\n",
    "- Return top 5\n",
    "- Cost: Filtering + embedding lookup\n",
    "\n",
    "#### Strategy D: Full Context with Cache\n",
    "- Load entire document into LLM context\n",
    "- Use prompt caching for cost reduction\n",
    "- Let LLM select 5 most relevant sections\n",
    "- Cost: Full LLM inference (reduced with caching)\n",
    "\n",
    "### 4. Evaluation Metrics\n",
    "For each question and strategy, calculate:\n",
    "- **Matches**: Number of retrieved TOCs that appear in golden source\n",
    "- **Precision@5**: Matches / 5\n",
    "- **Cost**: Based on API calls (embeddings, LLM tokens)\n",
    "- **Latency**: Time to retrieve results\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "class DocumentCategorizer:\n",
    "    def __init__(self, category_1_list, category_2_list, llm_model):\n",
    "        self.categories_1 = category_1_list\n",
    "        self.categories_2 = category_2_list\n",
    "        self.llm = llm_model\n",
    "        \n",
    "    def categorize_text(self, text):\n",
    "        # Return (category_1, category_2)\n",
    "        # Option 1: LLM-based categorization\n",
    "        prompt = f\"\"\"\n",
    "        Categorize this text:\n",
    "        \"{text}\"\n",
    "        \n",
    "        Category 1 options: {self.categories_1}\n",
    "        Category 2 options: {self.categories_2}\n",
    "        \n",
    "        Return: (category_1, category_2)\n",
    "        \"\"\"\n",
    "        return self.llm.predict(prompt)\n",
    "        \n",
    "    def categorize_dataset(self, document_df):\n",
    "        # Add category columns to dataframe\n",
    "        for idx, row in document_df.iterrows():\n",
    "            cat1, cat2 = self.categorize_text(row['Text'])\n",
    "            document_df.loc[idx, 'Category_1'] = cat1\n",
    "            document_df.loc[idx, 'Category_2'] = cat2\n",
    "        return document_df\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, categorized_document_df, embedding_model):\n",
    "        # Initialize embeddings and categories\n",
    "        \n",
    "    def categorize_question(self, question):\n",
    "        # Return (category_1, category_2)\n",
    "        \n",
    "    def retrieve_by_embedding(self, question, k=5):\n",
    "        # Strategy A implementation\n",
    "        \n",
    "    def retrieve_by_category(self, question, k=5):\n",
    "        # Strategy B implementation\n",
    "        \n",
    "    def retrieve_hybrid(self, question, k=5):\n",
    "        # Strategy C implementation\n",
    "        \n",
    "    def retrieve_full_context(self, question, k=5):\n",
    "        # Strategy D implementation\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, golden_source_df):\n",
    "        # Load golden source\n",
    "        \n",
    "    def evaluate_strategy(self, strategy_results, question_id):\n",
    "        # Calculate precision and matches\n",
    "        \n",
    "    def generate_report(self, all_results):\n",
    "        # Create summary and detailed tables\n",
    "```\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "### 1. Categorized Document Dataset (Excel)\n",
    "**File:** `document_dataset_categorized.xlsx`\n",
    "- Original columns plus Category_1 and Category_2\n",
    "- Export for verification and manual correction if needed\n",
    "\n",
    "### 2. Summary Table\n",
    "\n",
    "| Strategy | Avg Precision | Total Cost | Avg Latency | Best For |\n",
    "|----------|---------------|------------|-------------|----------|\n",
    "| A: Embedding | 0.62 | 0.0010 | 50ms | Semantic search |\n",
    "| B: Categories | 0.48 | 0.0000 | 10ms | Quick filtering |\n",
    "| C: Hybrid | 0.74 |  0.0010 | 60ms | Balanced approach |\n",
    "| D: Full Context | 0.88 | 0.0500 | 500ms | High accuracy |\n",
    "\n",
    "### 3. Detailed Results (Excel Export)\n",
    "- **Sheet 1**: Summary statistics\n",
    "- **Sheet 2**: Per-question results for all strategies\n",
    "- **Sheet 3**: Cost breakdown by component\n",
    "- **Sheet 4**: Retrieved TOCs vs Golden TOCs comparison\n",
    "- **Sheet 5**: Categorized documents\n",
    "\n",
    "### 4. Cost Breakdown Example\n",
    "```\n",
    "Initial Setup:\n",
    "- Document categorization: 100 docs × $0.001 = $0.10 (one-time)\n",
    "- Document embeddings: 100 docs × $0.0001 = $0.01 (one-time)\n",
    "\n",
    "Per-Question Costs:\n",
    "- Question categorization: $0.001\n",
    "- Strategy A: $0.0001 (embedding lookup)\n",
    "- Strategy B: $0.0000 (metadata only)\n",
    "- Strategy C: $0.0001 (filtering + embedding)\n",
    "- Strategy D: $0.005 (full context) or $0.0005 (cached)\n",
    "```\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "### Required Libraries\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import openpyxl\n",
    "from tqdm import tqdm  # For progress bars during categorization\n",
    "```\n",
    "\n",
    "### Configuration Parameters\n",
    "```python\n",
    "config = {\n",
    "    \"embedding_model\": \"text-embedding-ada-002\",\n",
    "    \"llm_model\": \"gpt-4\",\n",
    "    \"retrieval_k\": 5,\n",
    "    \"categories_1\": [\"Fundamentals\", \"Algorithms\", \"Data Processing\", \"Advanced Topics\"],\n",
    "    \"categories_2\": [\"Beginner\", \"Intermediate\", \"Advanced\", \"Expert\"],\n",
    "    \"cost_per_embedding\": 0.0001,\n",
    "    \"cost_per_1k_tokens\": 0.03,\n",
    "    \"use_cache\": True,\n",
    "    \"categorization_method\": \"llm\"  # Options: \"llm\", \"embedding\", \"keyword\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "```python\n",
    "# Step 1: Load and categorize documents\n",
    "documents_df = pd.read_excel(\"document_dataset.xlsx\")\n",
    "golden_df = pd.read_excel(\"golden_source.xlsx\")\n",
    "\n",
    "categorizer = DocumentCategorizer(\n",
    "    config[\"categories_1\"], \n",
    "    config[\"categories_2\"], \n",
    "    ChatOpenAI(model=config[\"llm_model\"])\n",
    ")\n",
    "\n",
    "print(\"Categorizing documents...\")\n",
    "categorized_df = categorizer.categorize_dataset(documents_df)\n",
    "categorized_df.to_excel(\"document_dataset_categorized.xlsx\", index=False)\n",
    "\n",
    "# Step 2: Initialize retrieval system\n",
    "retriever = DocumentRetriever(categorized_df, config)\n",
    "evaluator = Evaluator(golden_df)\n",
    "\n",
    "# Step 3: Run evaluation\n",
    "results = {}\n",
    "for idx, row in golden_df.iterrows():\n",
    "    question_id = row['Question_ID']\n",
    "    question = row['Question_Text']\n",
    "    \n",
    "    results[question_id] = {\n",
    "        'A': retriever.retrieve_by_embedding(question),\n",
    "        'B': retriever.retrieve_by_category(question),\n",
    "        'C': retriever.retrieve_hybrid(question),\n",
    "        'D': retriever.retrieve_full_context(question)\n",
    "    }\n",
    "\n",
    "# Step 4: Generate report\n",
    "evaluator.generate_report(results)\n",
    "```\n",
    "\n",
    "## Deliverables\n",
    "1. Categorized document dataset (Excel file)\n",
    "2. Working Python script with:\n",
    "   - Automatic document categorization\n",
    "   - Four retrieval strategies\n",
    "   - Evaluation framework\n",
    "3. Excel report with comprehensive evaluation metrics\n",
    "4. Cost analysis including:\n",
    "   - One-time setup costs (categorization, embeddings)\n",
    "   - Per-query costs for each strategy\n",
    "5. Recommendations based on accuracy/cost tradeoffs\n",
    "\n",
    "## Notes\n",
    "- The categorization step is crucial for Strategy B and C\n",
    "- Consider manual verification of categories for critical documents\n",
    "- Category definitions should be clear and mutually exclusive\n",
    "- Save categorized dataset for reuse in future experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60df3c7-3c7d-4a2f-acc8-a42b7eda642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4.1-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75387c15-1e7e-4b99-82b2-b08dcbf0d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "\n",
    "config = {\n",
    "    \"embedding_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4-turbo\",\n",
    "    \"retrieval_k\": 5,\n",
    "    \"categories_1\": [\"Fundamentals\", \"Algorithms\", \"Data Processing\", \"Advanced Topics\", \"Evaluation\"],\n",
    "    \"categories_2\": [\"Beginner\", \"Intermediate\", \"Advanced\", \"Expert\"],\n",
    "    \"cost_embedding_per_1m_tokens\": 0.02,\n",
    "    \"cost_llm_input_per_1m_tokens\": 5.00,\n",
    "    \"cost_llm_output_per_1m_tokens\": 15.00,\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "\n",
    "# --- Helper Functions & Setup ---\n",
    "\n",
    "def create_dummy_files():\n",
    "    \"\"\"Creates dummy Excel files for demonstration if they don't exist.\"\"\"\n",
    "    doc_file = \"document_dataset.xlsx\"\n",
    "    golden_file = \"golden_source.xlsx\"\n",
    "    if not os.path.exists(doc_file):\n",
    "        print(f\"Creating dummy file: {doc_file}\")\n",
    "        pd.DataFrame({\n",
    "            \"TOC_Number\": [\"1.1\", \"1.2\", \"1.3\", \"2.1\", \"2.2\", \"2.3\", \"3.1\", \"3.2\", \"3.3\", \"4.1\"],\n",
    "            \"Text\": [\n",
    "                \"Introduction to machine learning, covering basic concepts like variables and data types.\",\n",
    "                \"Exploring supervised learning algorithms, including linear regression and logistic regression.\",\n",
    "                \"An overview of unsupervised learning techniques such as k-means clustering.\",\n",
    "                \"Deep dive into data preprocessing: handling missing values, scaling features.\",\n",
    "                \"Methods for model evaluation: confusion matrix, precision, recall, and F1-score.\",\n",
    "                \"Feature engineering strategies to improve model performance.\",\n",
    "                \"A look at neural networks and their fundamental architecture.\",\n",
    "                \"Advanced neural network types: Convolutional Neural Networks (CNNs) for images.\",\n",
    "                \"Recurrent Neural Networks (RNNs) for sequence data.\",\n",
    "                \"Understanding transfer learning and fine-tuning pre-trained models.\"\n",
    "            ]\n",
    "        }).to_excel(doc_file, index=False)\n",
    "    if not os.path.exists(golden_file):\n",
    "        print(f\"Creating dummy file: {golden_file}\")\n",
    "        pd.DataFrame({\n",
    "            \"Question_ID\": [f\"Q{i}\" for i in range(1, 6)],\n",
    "            \"Question_Text\": [\n",
    "                \"How do I evaluate my machine learning model?\", \"What are the main types of neural networks?\",\n",
    "                \"How should I start learning about ML?\", \"What is the difference between supervised and unsupervised learning?\",\n",
    "                \"How to prepare data for a model?\"\n",
    "            ],\n",
    "            \"TOC_1\": [\"2.2\", \"3.2\", \"1.1\", \"1.2\", \"2.1\"], \"TOC_2\": [\"2.3\", \"3.1\", \"1.2\", \"1.3\", \"2.3\"],\n",
    "            \"TOC_3\": [\"1.2\", \"3.3\", \"2.1\", \"3.1\", \"4.1\"], \"TOC_4\": [\"4.1\", \"4.1\", \"1.3\", \"2.2\", \"1.2\"],\n",
    "            \"TOC_5\": [\"1.1\", \"1.1\", \"3.1\", \"2.1\", \"3.2\"],\n",
    "        }).to_excel(golden_file, index=False)\n",
    "\n",
    "# --- System Components ---\n",
    "\n",
    "class CostTracker:\n",
    "    def __init__(self, config):\n",
    "        self.config, self.total_cost, self.cost_breakdown = config, 0, {\n",
    "            \"setup_categorization\": 0, \"setup_embedding\": 0, \"query_categorization\": 0,\n",
    "            \"query_embedding\": 0, \"query_llm_context\": 0\n",
    "        }\n",
    "    def _calculate_cost(self, tokens, type):\n",
    "        if type == \"embedding\": return (tokens / 1_000_000) * self.config['cost_embedding_per_1m_tokens']\n",
    "        if type == \"llm_input\": return (tokens / 1_000_000) * self.config['cost_llm_input_per_1m_tokens']\n",
    "        if type == \"llm_output\": return (tokens / 1_000_000) * self.config['cost_llm_output_per_1m_tokens']\n",
    "        return 0\n",
    "        \n",
    "    def add_cost(self, tokens, type, component):\n",
    "        cost = self._calculate_cost(tokens, type)\n",
    "        self.total_cost += cost\n",
    "        if component in self.cost_breakdown: self.cost_breakdown[component] += cost\n",
    "        return cost\n",
    "        \n",
    "    def get_summary(self): return {\"total_cost\": self.total_cost, \"breakdown\": self.cost_breakdown}\n",
    "\n",
    "class DocumentCategorizer:\n",
    "    def __init__(self, category_1_list, category_2_list, llm, cost_tracker):\n",
    "        self.llm, self.cost_tracker = llm, cost_tracker\n",
    "        class Categories(BaseModel):\n",
    "            category_1: str = Field(description=f\"The topic from the list: {category_1_list}\")\n",
    "            category_2: str = Field(description=f\"The level from the list: {category_2_list}\")\n",
    "        self.parser = PydanticOutputParser(pydantic_object=Categories)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"Analyze the text and assign categories.\\n{format_instructions}\\nText: \\\"{text}\\\"\",\n",
    "            input_variables=[\"text\"], partial_variables={\"format_instructions\": self.parser.get_format_instructions()},\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm | self.parser\n",
    "        \n",
    "    def categorize_text(self, text: str):\n",
    "        try:\n",
    "            input_tokens, result = len(text) // 4, self.chain.invoke({\"text\": text})\n",
    "            output_tokens = len(str(result)) // 4\n",
    "            self.cost_tracker.add_cost(input_tokens, \"llm_input\", \"setup_categorization\")\n",
    "            self.cost_tracker.add_cost(output_tokens, \"llm_output\", \"setup_categorization\")\n",
    "            return result.category_1, result.category_2\n",
    "        except Exception as e: return \"Uncategorized\", \"Uncategorized\"\n",
    "            \n",
    "    def categorize_dataset(self, df: pd.DataFrame):\n",
    "        cats = [self.categorize_text(row['Text']) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Categorizing Documents\")]\n",
    "        return pd.concat([df, pd.DataFrame(cats, columns=[\"Category_1\", \"Category_2\"])], axis=1)\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, df, config, cost_tracker):\n",
    "        self.df, self.config, self.cost_tracker, self.llm_cache = df.copy(), config, cost_tracker, {}\n",
    "        self.embedding_model = OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "        self.llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "        self.categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], self.llm, cost_tracker)\n",
    "        print(\"Generating document embeddings...\")\n",
    "        texts, total_tokens = self.df['Text'].tolist(), sum(len(t)//4 for t in self.df['Text'])\n",
    "        self.cost_tracker.add_cost(total_tokens, 'embedding', 'setup_embedding')\n",
    "        self.df['embedding'] = self.embedding_model.embed_documents(texts)\n",
    "        \n",
    "    def _categorize_question(self, question, question_id):\n",
    "        if hasattr(self, '_cat_cache') and question_id in self._cat_cache: return self._cat_cache[question_id]\n",
    "        input_tokens, (cat1, cat2) = len(question)//4, self.categorizer.categorize_text(question)\n",
    "        output_tokens = len(cat1)//4 + len(cat2)//4\n",
    "        self.cost_tracker.add_cost(input_tokens, 'llm_input', 'query_categorization')\n",
    "        self.cost_tracker.add_cost(output_tokens, 'llm_output', 'query_categorization')\n",
    "        if not hasattr(self, '_cat_cache'): self._cat_cache = {}\n",
    "        self._cat_cache[question_id] = (cat1, cat2)\n",
    "        return cat1, cat2\n",
    "        \n",
    "    def retrieve(self, s_name, question, k, q_id):\n",
    "        start = time.time()\n",
    "        if s_name == 'A': tocs, cost = self.retrieve_by_embedding(question, k)\n",
    "        elif s_name == 'B': tocs, cost = self.retrieve_by_category(question, k, q_id)\n",
    "        elif s_name == 'C': tocs, cost = self.retrieve_hybrid(question, k, q_id)\n",
    "        elif s_name == 'D': tocs, cost = self.retrieve_full_context(question, k)\n",
    "        else: raise ValueError(f\"Unknown strategy: {s_name}\")\n",
    "        return tocs, (time.time() - start) * 1000, cost\n",
    "        \n",
    "    def retrieve_by_embedding(self, question, k):\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        sims = cosine_similarity([q_emb], np.array(self.df['embedding'].tolist()))[0]\n",
    "        return self.df.iloc[np.argsort(sims)[::-1][:k]]['TOC_Number'].tolist(), cost\n",
    "        \n",
    "    def retrieve_by_category(self, question, k, q_id):\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        df = self.df[(self.df['Category_1'] == cat1) & (self.df['Category_2'] == cat2)]\n",
    "        if df.empty: df = self.df[self.df['Category_1'] == cat1]\n",
    "        return df['TOC_Number'].head(k).tolist(), 0\n",
    "        \n",
    "    def retrieve_hybrid(self, question, k, q_id):\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        # --- THIS IS THE CORRECTED LINE ---\n",
    "        df = pd.concat([\n",
    "            self.df[(self.df['Category_1']==cat1)&(self.df['Category_2']==cat2)], \n",
    "            self.df[self.df['Category_1']==cat1]\n",
    "        ]).drop_duplicates(subset=['TOC_Number']).reset_index(drop=True)\n",
    "        # --- END OF CORRECTION ---\n",
    "        if df.empty: return [], cost\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        df['sim'] = cosine_similarity([q_emb], np.array(df['embedding'].tolist()))[0]\n",
    "        return df.sort_values('sim', ascending=False).head(k)['TOC_Number'].tolist(), cost\n",
    "        \n",
    "    def retrieve_full_context(self, question, k):\n",
    "        if self.config['use_cache'] and question in self.llm_cache: return self.llm_cache[question][0], 0\n",
    "        context_str = \"\\n\".join([f\"TOC {row['TOC_Number']}: {row['Text']}\" for _, row in self.df.iterrows()])\n",
    "        prompt = f\"\"\"Given the document context below, identify the TOP {k} `TOC_Number`s most relevant to the user's question. Return only a comma-separated list of TOC numbers (e.g., 1.1, 2.3, 3.2).\n",
    "CONTEXT:\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "QUESTION: \"{question}\"\n",
    "Relevant TOC_Numbers:\n",
    "\"\"\"\n",
    "        res = self.llm.invoke(prompt).content.strip()\n",
    "        tocs = [t.strip() for t in res.split(',')]\n",
    "        cost = self.cost_tracker.add_cost(len(prompt)//4, 'llm_input', 'query_llm_context') + \\\n",
    "               self.cost_tracker.add_cost(len(res)//4, 'llm_output', 'query_llm_context')\n",
    "        if self.config['use_cache']: self.llm_cache[question] = (tocs, cost)\n",
    "        return tocs, cost\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, golden_source_df, k):\n",
    "        self.k = k\n",
    "        self.golden_df = golden_source_df\n",
    "        toc_cols = [f'TOC_{i}' for i in range(1, 6)]\n",
    "        self.golden_map = {row['Question_ID']: set(row[toc_cols].astype(str).values) for _, row in self.golden_df.iterrows()}\n",
    "            \n",
    "    def evaluate_run(self, retrieved_tocs, question_id):\n",
    "        golden_tocs = self.golden_map.get(question_id, set())\n",
    "        matches = len(set(retrieved_tocs).intersection(golden_tocs))\n",
    "        precision = matches / self.k if self.k > 0 else 0\n",
    "        return {'matches': matches, 'precision': precision, 'retrieved_tocs': \", \".join(map(str, retrieved_tocs)), 'golden_tocs': \", \".join(map(str, sorted(list(golden_tocs))))}\n",
    "\n",
    "    def generate_report(self, all_results, cost_tracker, categorized_df):\n",
    "        print(\"\\nGenerating final report...\")\n",
    "        report_filename = \"retrieval_evaluation_report.xlsx\"\n",
    "        strategy_map = {'A': 'A: Embedding', 'B': 'B: Categories', 'C': 'C: Hybrid', 'D': 'D: Full Context'}\n",
    "\n",
    "        # 1. Per-Question Accuracy Summary\n",
    "        per_q_summary_data = []\n",
    "        for q_id, q_results in all_results.items():\n",
    "            golden_tocs_set = self.golden_map.get(q_id, set())\n",
    "            golden_tocs_str = \", \".join(map(str, sorted(list(golden_tocs_set))))\n",
    "            for s_code, result in q_results.items():\n",
    "                if s_code == 'categorization_cost': continue\n",
    "                eval_metrics = self.evaluate_run(result['tocs'], q_id)\n",
    "                per_q_summary_data.append({\n",
    "                    'Question_ID': q_id,\n",
    "                    'Strategy': strategy_map[s_code],\n",
    "                    'Golden TOCs': golden_tocs_str,\n",
    "                    'Retrieved TOCs': eval_metrics['retrieved_tocs'],\n",
    "                    'Overlap %': f\"{eval_metrics['precision']:.0%}\"\n",
    "                })\n",
    "        per_q_summary_df = pd.DataFrame(per_q_summary_data)\n",
    "\n",
    "        # 2. Detailed Per-Question Metrics\n",
    "        detailed_metrics_data = []\n",
    "        for q_id, q_results in all_results.items():\n",
    "            row = {'Question_ID': q_id, 'Question_Text': self.golden_df[self.golden_df['Question_ID'] == q_id]['Question_Text'].iloc[0]}\n",
    "            q_cat_cost = q_results.get('categorization_cost', 0)\n",
    "            for s_code, result in q_results.items():\n",
    "                if s_code == 'categorization_cost': continue\n",
    "                eval_metrics = self.evaluate_run(result['tocs'], q_id)\n",
    "                final_cost = result['cost'] + (q_cat_cost if s_code in ['B', 'C'] else 0)\n",
    "                row[f'{s_code}_Matches'] = eval_metrics['matches']\n",
    "                row[f'{s_code}_Precision'] = eval_metrics['precision']\n",
    "                row[f'{s_code}_Latency(ms)'] = result['latency']\n",
    "                row[f'{s_code}_Cost($)'] = final_cost\n",
    "            detailed_metrics_data.append(row)\n",
    "        detailed_metrics_df = pd.DataFrame(detailed_metrics_data)\n",
    "\n",
    "        # 3. Strategy-Level Summary\n",
    "        summary_data = []\n",
    "        for s_code, s_name in strategy_map.items():\n",
    "            summary_data.append({\n",
    "                'Strategy': s_name,\n",
    "                'Avg Precision': f\"{detailed_metrics_df[f'{s_code}_Precision'].mean():.2%}\",\n",
    "                'Total Query Cost ($)': f\"{detailed_metrics_df[f'{s_code}_Cost($)'].sum():.6f}\",\n",
    "                'Avg Latency (ms)': f\"{detailed_metrics_df[f'{s_code}_Latency(ms)'].mean():.2f}\"\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # 4. Cost Breakdown\n",
    "        costs = cost_tracker.get_summary()\n",
    "        cost_df = pd.DataFrame({\n",
    "            'Component': [\n",
    "                'Setup: Document Categorization', 'Setup: Document Embeddings', '---',\n",
    "                'Total Query Costs (Aggregated)', '---', 'Total Estimated Cost (Setup + Query)'\n",
    "            ],\n",
    "            'Cost ($)': [\n",
    "                f\"{costs['breakdown']['setup_categorization']:.6f}\", f\"{costs['breakdown']['setup_embedding']:.6f}\", '---',\n",
    "                f\"{costs['total_cost'] - costs['breakdown']['setup_categorization'] - costs['breakdown']['setup_embedding']:.6f}\", '---',\n",
    "                f\"{costs['total_cost']:.6f}\"\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # 5. Write all DataFrames to a multi-sheet Excel file\n",
    "        with pd.ExcelWriter(report_filename, engine='openpyxl') as writer:\n",
    "            per_q_summary_df.to_excel(writer, sheet_name='Per-Question Accuracy Summary', index=False)\n",
    "            summary_df.to_excel(writer, sheet_name='Strategy-Level Summary', index=False)\n",
    "            detailed_metrics_df.to_excel(writer, sheet_name='Detailed Per-Question Metrics', index=False)\n",
    "            cost_df.to_excel(writer, sheet_name='Cost Breakdown', index=False)\n",
    "            categorized_df.drop(columns=['embedding'], errors='ignore').to_excel(\n",
    "                writer, sheet_name='Categorized Documents', index=False\n",
    "            )\n",
    "            \n",
    "        print(\"\\n--- Evaluation Report ---\")\n",
    "        print(\"Per-Question Accuracy Summary (Top 5 rows):\")\n",
    "        print(per_q_summary_df.head().to_string(index=False))\n",
    "        print(\"\\nStrategy-Level Summary:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(f\"\\nFull report with 5 sheets saved to '{report_filename}'\")\n",
    "        print(f\"Total estimated cost for this run: ${costs['total_cost']:.4f}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    create_dummy_files()\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable not set. Please create a .env file.\")\n",
    "\n",
    "    cost_tracker = CostTracker(config)\n",
    "    llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "    docs_df = pd.read_excel(\"document_dataset.xlsx\")\n",
    "    golden_df = pd.read_excel(\"golden_source.xlsx\")\n",
    "\n",
    "    categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], llm, cost_tracker)\n",
    "    categorized_df = categorizer.categorize_dataset(docs_df)\n",
    "    categorized_df.to_excel(\"document_dataset_categorized.xlsx\", index=False)\n",
    "    print(\"Categorized documents saved.\")\n",
    "\n",
    "    retriever = DocumentRetriever(categorized_df, config, cost_tracker)\n",
    "    \n",
    "    all_results = {}\n",
    "    print(\"\\nRunning retrieval strategies for all questions...\")\n",
    "    for _, row in tqdm(golden_df.iterrows(), total=len(golden_df), desc=\"Evaluating Questions\"):\n",
    "        q_id, question = row['Question_ID'], row['Question_Text']\n",
    "        cost_tracker.cost_breakdown['query_categorization'] = 0\n",
    "        \n",
    "        all_results[q_id] = {\n",
    "            s_code: {'tocs': t, 'latency': l, 'cost': c}\n",
    "            for s_code, (t, l, c) in zip(\n",
    "                ['A', 'B', 'C', 'D'],\n",
    "                [retriever.retrieve(s, question, config[\"retrieval_k\"], q_id) for s in ['A', 'B', 'C', 'D']]\n",
    "            )\n",
    "        }\n",
    "        all_results[q_id]['categorization_cost'] = cost_tracker.cost_breakdown['query_categorization']\n",
    "        if hasattr(retriever, '_cat_cache'): retriever._cat_cache.clear()\n",
    "\n",
    "    evaluator = Evaluator(golden_df, config[\"retrieval_k\"])\n",
    "    evaluator.generate_report(all_results, cost_tracker, retriever.df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
