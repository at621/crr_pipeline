{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16332433-4ad6-4147-bd38-8f88fd93ab2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0833958-08b2-4c99-9b61-ddb1538b1482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401d013c-4e34-436e-9607-be75fdf7227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document dataset from parquet file...\n",
      "Available columns in parquet file: ['Part', 'Title', 'Chapter', 'Section', 'Subsection', 'Part_Heading', 'Title_Heading', 'Chapter_Heading', 'Section_Heading', 'Subsection_Heading', 'Token_Count', 'Ends_With_Dot', 'Article_Number', 'Article_Heading', 'Text', 'Text_With_Pagebreaks', 'combined_text', 'embedding']\n",
      "Number of documents: 20\n",
      "Converting embeddings from numpy arrays to lists...\n",
      "Sample text (first 200 chars): Article_Heading: Article 1 - Scope...\n",
      "Embedding dimension: 1536\n",
      "\n",
      "Categorizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorizing Documents: 100%|██████████████████████████████████████████████████████████| 20/20 [00:17<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorized documents saved to parquet.\n",
      "Using existing embeddings from dataframe...\n",
      "\n",
      "Running retrieval strategies for all questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Questions: 100%|██████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating final report...\n",
      "\n",
      "--- Evaluation Report ---\n",
      "Per-Question Accuracy Summary (Top 5 rows):\n",
      "Question_ID        Strategy             Golden TOCs           Retrieved TOCs Overlap %\n",
      "         Q1    A: Embedding 1.1, 1.2, 2.2, 2.3, 4.1  1.8, 2.1, 1.2, 2.4, 2.6       20%\n",
      "         Q1   B: Categories 1.1, 1.2, 2.2, 2.3, 4.1  1.1, 1.2, 1.3, 1.4, 1.5       40%\n",
      "         Q1       C: Hybrid 1.1, 1.2, 2.2, 2.3, 4.1  1.8, 2.1, 1.2, 2.4, 2.6       20%\n",
      "         Q1 D: Full Context 1.1, 1.2, 2.2, 2.3, 4.1  1.6, 1.5, 1.4, 1.3, 1.2       20%\n",
      "         Q2    A: Embedding 1.1, 3.1, 3.2, 3.3, 4.1 2.5, 2.1, 1.10, 1.6, 2.8        0%\n",
      "\n",
      "Strategy-Level Summary:\n",
      "       Strategy Avg Precision Total Query Cost ($) Avg Latency (ms)\n",
      "   A: Embedding        24.00%             0.000001           356.85\n",
      "  B: Categories        28.00%             0.000010           674.10\n",
      "      C: Hybrid        16.00%             0.000011           319.35\n",
      "D: Full Context        24.00%             0.007716           960.11\n",
      "\n",
      "Full report with 5 sheets saved to 'retrieval_evaluation_report.xlsx'\n",
      "Total estimated cost for this run: $0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Suppress LangChain tracer warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Error in LangChainTracer\")\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "\n",
    "# Disable LangChain tracing to avoid serialization errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"false\"\n",
    "\n",
    "# Expected input file structure:\n",
    "# - data/df_with_embeddings.parquet with columns:\n",
    "#   - 'combined_text': the text content\n",
    "#   - 'embedding': OpenAI text-embedding-3-small vectors\n",
    "\n",
    "config = {\n",
    "    \"embedding_model\": \"text-embedding-3-small\",\n",
    "    \"llm_model\": \"gpt-4o-mini\",  # Changed to gpt-4o-mini for cost efficiency\n",
    "    \"retrieval_k\": 5,\n",
    "    \"categories_1\": [\"IRB\", \"SA\"],  # Updated with real categories\n",
    "    \"categories_2\": [\"PD\", \"LGD\", \"In-default LGD\"],  # Updated with real categories\n",
    "    \"cost_embedding_per_1m_tokens\": 0.02,\n",
    "    \"cost_llm_input_per_1m_tokens\": 0.15,  # Updated for gpt-4o-mini pricing\n",
    "    \"cost_llm_output_per_1m_tokens\": 0.60,  # Updated for gpt-4o-mini pricing\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "\n",
    "# --- Helper Functions & Setup ---\n",
    "\n",
    "def create_dummy_golden_file():\n",
    "    \"\"\"Creates dummy golden source file for demonstration if it doesn't exist.\"\"\"\n",
    "    golden_file = \"golden_source.xlsx\"\n",
    "    if not os.path.exists(golden_file):\n",
    "        print(f\"Creating dummy file: {golden_file}\")\n",
    "        # Adjusted TOC numbers to match the format we'll generate (row_index//10+1).(row_index%10+1)\n",
    "        pd.DataFrame({\n",
    "            \"Question_ID\": [f\"Q{i}\" for i in range(1, 6)],\n",
    "            \"Question_Text\": [\n",
    "                \"What are the IRB requirements for PD models?\",\n",
    "                \"How is LGD calculated under the SA approach?\",\n",
    "                \"What is the difference between PD and in-default LGD?\",\n",
    "                \"What are the key components of IRB models?\",\n",
    "                \"How does SA handle LGD estimation?\"\n",
    "            ],\n",
    "            \"TOC_1\": [\"1.1\", \"1.2\", \"1.3\", \"1.4\", \"1.5\"],\n",
    "            \"TOC_2\": [\"2.1\", \"2.2\", \"2.3\", \"2.4\", \"2.5\"],\n",
    "            \"TOC_3\": [\"3.1\", \"3.2\", \"3.3\", \"3.4\", \"3.5\"],\n",
    "            \"TOC_4\": [\"4.1\", \"4.2\", \"4.3\", \"4.4\", \"4.5\"],\n",
    "            \"TOC_5\": [\"5.1\", \"5.2\", \"5.3\", \"5.4\", \"5.5\"],\n",
    "        }).to_excel(golden_file, index=False)\n",
    "        print(f\"Note: Please update {golden_file} with actual TOC references from your documents.\")\n",
    "\n",
    "# --- System Components ---\n",
    "\n",
    "class CostTracker:\n",
    "    def __init__(self, config):\n",
    "        self.config, self.total_cost, self.cost_breakdown = config, 0, {\n",
    "            \"setup_categorization\": 0, \"setup_embedding\": 0, \"query_categorization\": 0,\n",
    "            \"query_embedding\": 0, \"query_llm_context\": 0\n",
    "        }\n",
    "    def _calculate_cost(self, tokens, type):\n",
    "        if type == \"embedding\": return (tokens / 1_000_000) * self.config['cost_embedding_per_1m_tokens']\n",
    "        if type == \"llm_input\": return (tokens / 1_000_000) * self.config['cost_llm_input_per_1m_tokens']\n",
    "        if type == \"llm_output\": return (tokens / 1_000_000) * self.config['cost_llm_output_per_1m_tokens']\n",
    "        return 0\n",
    "        \n",
    "    def add_cost(self, tokens, type, component):\n",
    "        cost = self._calculate_cost(tokens, type)\n",
    "        self.total_cost += cost\n",
    "        if component in self.cost_breakdown: self.cost_breakdown[component] += cost\n",
    "        return cost\n",
    "        \n",
    "    def get_summary(self): return {\"total_cost\": self.total_cost, \"breakdown\": self.cost_breakdown}\n",
    "\n",
    "class DocumentCategorizer:\n",
    "    def __init__(self, category_1_list, category_2_list, llm, cost_tracker):\n",
    "        self.llm, self.cost_tracker = llm, cost_tracker\n",
    "        class Categories(BaseModel):\n",
    "            category_1: str = Field(description=f\"The approach/framework from the list: {category_1_list}\")\n",
    "            category_2: str = Field(description=f\"The risk parameter from the list: {category_2_list}\")\n",
    "        self.parser = PydanticOutputParser(pydantic_object=Categories)\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"Analyze the text and assign categories.\\n{format_instructions}\\nText: \\\"{text}\\\"\",\n",
    "            input_variables=[\"text\"], partial_variables={\"format_instructions\": self.parser.get_format_instructions()},\n",
    "        )\n",
    "        self.chain = self.prompt | self.llm | self.parser\n",
    "        \n",
    "    def categorize_text(self, text: str):\n",
    "        try:\n",
    "            # Ensure text is a string\n",
    "            text = str(text) if not isinstance(text, str) else text\n",
    "            # Limit text length to avoid token limits\n",
    "            text = text[:2000] if len(text) > 2000 else text\n",
    "            \n",
    "            input_tokens, result = len(text) // 4, self.chain.invoke({\"text\": text})\n",
    "            output_tokens = len(str(result)) // 4\n",
    "            self.cost_tracker.add_cost(input_tokens, \"llm_input\", \"setup_categorization\")\n",
    "            self.cost_tracker.add_cost(output_tokens, \"llm_output\", \"setup_categorization\")\n",
    "            return result.category_1, result.category_2\n",
    "        except Exception as e: \n",
    "            print(f\"Error categorizing text: {e}\")\n",
    "            return \"Uncategorized\", \"Uncategorized\"\n",
    "            \n",
    "    def categorize_dataset(self, df: pd.DataFrame):\n",
    "        # Make a copy to avoid modifying the original\n",
    "        df_copy = df.copy()\n",
    "        categories = []\n",
    "        \n",
    "        for idx, row in tqdm(df_copy.iterrows(), total=len(df_copy), desc=\"Categorizing Documents\"):\n",
    "            # Extract text as string\n",
    "            text = str(row['Text']) if pd.notna(row['Text']) else \"\"\n",
    "            cat1, cat2 = self.categorize_text(text)\n",
    "            categories.append((cat1, cat2))\n",
    "        \n",
    "        df_copy['Category_1'] = [cat[0] for cat in categories]\n",
    "        df_copy['Category_2'] = [cat[1] for cat in categories]\n",
    "        return df_copy\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, df, config, cost_tracker, skip_embedding_generation=False):\n",
    "        self.df, self.config, self.cost_tracker, self.llm_cache = df.copy(), config, cost_tracker, {}\n",
    "        self.embedding_model = OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "        self.llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "        self.categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], self.llm, cost_tracker)\n",
    "        \n",
    "        if not skip_embedding_generation:\n",
    "            print(\"Generating document embeddings...\")\n",
    "            texts, total_tokens = self.df['Text'].tolist(), sum(len(t)//4 for t in self.df['Text'])\n",
    "            self.cost_tracker.add_cost(total_tokens, 'embedding', 'setup_embedding')\n",
    "            self.df['embedding'] = self.embedding_model.embed_documents(texts)\n",
    "        else:\n",
    "            print(\"Using existing embeddings from dataframe...\")\n",
    "            # Ensure embeddings are in the right format (list of floats)\n",
    "            if 'embedding' in self.df.columns:\n",
    "                self.df['embedding'] = self.df['embedding'].apply(lambda x: x if isinstance(x, list) else list(x))\n",
    "        \n",
    "    def _categorize_question(self, question, question_id):\n",
    "        if hasattr(self, '_cat_cache') and question_id in self._cat_cache: return self._cat_cache[question_id]\n",
    "        input_tokens, (cat1, cat2) = len(question)//4, self.categorizer.categorize_text(question)\n",
    "        output_tokens = len(cat1)//4 + len(cat2)//4\n",
    "        self.cost_tracker.add_cost(input_tokens, 'llm_input', 'query_categorization')\n",
    "        self.cost_tracker.add_cost(output_tokens, 'llm_output', 'query_categorization')\n",
    "        if not hasattr(self, '_cat_cache'): self._cat_cache = {}\n",
    "        self._cat_cache[question_id] = (cat1, cat2)\n",
    "        return cat1, cat2\n",
    "        \n",
    "    def retrieve(self, s_name, question, k, q_id):\n",
    "        start = time.time()\n",
    "        if s_name == 'A': tocs, cost = self.retrieve_by_embedding(question, k)\n",
    "        elif s_name == 'B': tocs, cost = self.retrieve_by_category(question, k, q_id)\n",
    "        elif s_name == 'C': tocs, cost = self.retrieve_hybrid(question, k, q_id)\n",
    "        elif s_name == 'D': tocs, cost = self.retrieve_full_context(question, k)\n",
    "        else: raise ValueError(f\"Unknown strategy: {s_name}\")\n",
    "        return tocs, (time.time() - start) * 1000, cost\n",
    "        \n",
    "    def retrieve_by_embedding(self, question, k):\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        sims = cosine_similarity([q_emb], np.array(self.df['embedding'].tolist()))[0]\n",
    "        return self.df.iloc[np.argsort(sims)[::-1][:k]]['TOC_Number'].tolist(), cost\n",
    "        \n",
    "    def retrieve_by_category(self, question, k, q_id):\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        df = self.df[(self.df['Category_1'] == cat1) & (self.df['Category_2'] == cat2)]\n",
    "        if df.empty: df = self.df[self.df['Category_1'] == cat1]\n",
    "        return df['TOC_Number'].head(k).tolist(), 0\n",
    "        \n",
    "    def retrieve_hybrid(self, question, k, q_id):\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        # --- THIS IS THE CORRECTED LINE ---\n",
    "        df = pd.concat([\n",
    "            self.df[(self.df['Category_1']==cat1)&(self.df['Category_2']==cat2)], \n",
    "            self.df[self.df['Category_1']==cat1]\n",
    "        ]).drop_duplicates(subset=['TOC_Number']).reset_index(drop=True)\n",
    "        # --- END OF CORRECTION ---\n",
    "        if df.empty: return [], cost\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        df['sim'] = cosine_similarity([q_emb], np.array(df['embedding'].tolist()))[0]\n",
    "        return df.sort_values('sim', ascending=False).head(k)['TOC_Number'].tolist(), cost\n",
    "        \n",
    "    def retrieve_full_context(self, question, k):\n",
    "        if self.config['use_cache'] and question in self.llm_cache: return self.llm_cache[question][0], 0\n",
    "        context_str = \"\\n\".join([f\"TOC {row['TOC_Number']}: {row['Text']}\" for _, row in self.df.iterrows()])\n",
    "        prompt = f\"\"\"Given the document context below, identify the TOP {k} `TOC_Number`s most relevant to the user's question. Return only a comma-separated list of TOC numbers (e.g., 1.1, 2.3, 3.2).\n",
    "CONTEXT:\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "QUESTION: \"{question}\"\n",
    "Relevant TOC_Numbers:\n",
    "\"\"\"\n",
    "        res = self.llm.invoke(prompt).content.strip()\n",
    "        tocs = [t.strip() for t in res.split(',')]\n",
    "        cost = self.cost_tracker.add_cost(len(prompt)//4, 'llm_input', 'query_llm_context') + \\\n",
    "               self.cost_tracker.add_cost(len(res)//4, 'llm_output', 'query_llm_context')\n",
    "        if self.config['use_cache']: self.llm_cache[question] = (tocs, cost)\n",
    "        return tocs, cost\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, golden_source_df, k):\n",
    "        self.k = k\n",
    "        self.golden_df = golden_source_df\n",
    "        toc_cols = [f'TOC_{i}' for i in range(1, 6)]\n",
    "        self.golden_map = {row['Question_ID']: set(row[toc_cols].astype(str).values) for _, row in self.golden_df.iterrows()}\n",
    "            \n",
    "    def evaluate_run(self, retrieved_tocs, question_id):\n",
    "        golden_tocs = self.golden_map.get(question_id, set())\n",
    "        matches = len(set(retrieved_tocs).intersection(golden_tocs))\n",
    "        precision = matches / self.k if self.k > 0 else 0\n",
    "        return {'matches': matches, 'precision': precision, 'retrieved_tocs': \", \".join(map(str, retrieved_tocs)), 'golden_tocs': \", \".join(map(str, sorted(list(golden_tocs))))}\n",
    "\n",
    "    def generate_report(self, all_results, cost_tracker, categorized_df):\n",
    "        print(\"\\nGenerating final report...\")\n",
    "        report_filename = \"retrieval_evaluation_report.xlsx\"\n",
    "        strategy_map = {'A': 'A: Embedding', 'B': 'B: Categories', 'C': 'C: Hybrid', 'D': 'D: Full Context'}\n",
    "\n",
    "        # 1. Per-Question Accuracy Summary\n",
    "        per_q_summary_data = []\n",
    "        for q_id, q_results in all_results.items():\n",
    "            golden_tocs_set = self.golden_map.get(q_id, set())\n",
    "            golden_tocs_str = \", \".join(map(str, sorted(list(golden_tocs_set))))\n",
    "            for s_code, result in q_results.items():\n",
    "                if s_code == 'categorization_cost': continue\n",
    "                eval_metrics = self.evaluate_run(result['tocs'], q_id)\n",
    "                per_q_summary_data.append({\n",
    "                    'Question_ID': q_id,\n",
    "                    'Strategy': strategy_map[s_code],\n",
    "                    'Golden TOCs': golden_tocs_str,\n",
    "                    'Retrieved TOCs': eval_metrics['retrieved_tocs'],\n",
    "                    'Overlap %': f\"{eval_metrics['precision']:.0%}\"\n",
    "                })\n",
    "        per_q_summary_df = pd.DataFrame(per_q_summary_data)\n",
    "\n",
    "        # 2. Detailed Per-Question Metrics\n",
    "        detailed_metrics_data = []\n",
    "        for q_id, q_results in all_results.items():\n",
    "            row = {'Question_ID': q_id, 'Question_Text': self.golden_df[self.golden_df['Question_ID'] == q_id]['Question_Text'].iloc[0]}\n",
    "            q_cat_cost = q_results.get('categorization_cost', 0)\n",
    "            for s_code, result in q_results.items():\n",
    "                if s_code == 'categorization_cost': continue\n",
    "                eval_metrics = self.evaluate_run(result['tocs'], q_id)\n",
    "                final_cost = result['cost'] + (q_cat_cost if s_code in ['B', 'C'] else 0)\n",
    "                row[f'{s_code}_Matches'] = eval_metrics['matches']\n",
    "                row[f'{s_code}_Precision'] = eval_metrics['precision']\n",
    "                row[f'{s_code}_Latency(ms)'] = result['latency']\n",
    "                row[f'{s_code}_Cost($)'] = final_cost\n",
    "            detailed_metrics_data.append(row)\n",
    "        detailed_metrics_df = pd.DataFrame(detailed_metrics_data)\n",
    "\n",
    "        # 3. Strategy-Level Summary\n",
    "        summary_data = []\n",
    "        for s_code, s_name in strategy_map.items():\n",
    "            summary_data.append({\n",
    "                'Strategy': s_name,\n",
    "                'Avg Precision': f\"{detailed_metrics_df[f'{s_code}_Precision'].mean():.2%}\",\n",
    "                'Total Query Cost ($)': f\"{detailed_metrics_df[f'{s_code}_Cost($)'].sum():.6f}\",\n",
    "                'Avg Latency (ms)': f\"{detailed_metrics_df[f'{s_code}_Latency(ms)'].mean():.2f}\"\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # 4. Cost Breakdown\n",
    "        costs = cost_tracker.get_summary()\n",
    "        cost_df = pd.DataFrame({\n",
    "            'Component': [\n",
    "                'Setup: Document Categorization', 'Setup: Document Embeddings', '---',\n",
    "                'Total Query Costs (Aggregated)', '---', 'Total Estimated Cost (Setup + Query)'\n",
    "            ],\n",
    "            'Cost ($)': [\n",
    "                f\"{costs['breakdown']['setup_categorization']:.6f}\", f\"{costs['breakdown']['setup_embedding']:.6f}\", '---',\n",
    "                f\"{costs['total_cost'] - costs['breakdown']['setup_categorization'] - costs['breakdown']['setup_embedding']:.6f}\", '---',\n",
    "                f\"{costs['total_cost']:.6f}\"\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # 5. Write all DataFrames to a multi-sheet Excel file\n",
    "        with pd.ExcelWriter(report_filename, engine='openpyxl') as writer:\n",
    "            per_q_summary_df.to_excel(writer, sheet_name='Per-Question Accuracy Summary', index=False)\n",
    "            summary_df.to_excel(writer, sheet_name='Strategy-Level Summary', index=False)\n",
    "            detailed_metrics_df.to_excel(writer, sheet_name='Detailed Per-Question Metrics', index=False)\n",
    "            cost_df.to_excel(writer, sheet_name='Cost Breakdown', index=False)\n",
    "            categorized_df.drop(columns=['embedding'], errors='ignore').to_excel(\n",
    "                writer, sheet_name='Categorized Documents', index=False\n",
    "            )\n",
    "            \n",
    "        print(\"\\n--- Evaluation Report ---\")\n",
    "        print(\"Per-Question Accuracy Summary (Top 5 rows):\")\n",
    "        print(per_q_summary_df.head().to_string(index=False))\n",
    "        print(\"\\nStrategy-Level Summary:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(f\"\\nFull report with 5 sheets saved to '{report_filename}'\")\n",
    "        print(f\"Total estimated cost for this run: ${costs['total_cost']:.4f}\")\n",
    "\n",
    "# --- Jupyter Notebook Execution Code ---\n",
    "# Create dummy golden file if needed\n",
    "create_dummy_golden_file()\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set. Please create a .env file or set it in your notebook.\")\n",
    "\n",
    "# Initialize components\n",
    "cost_tracker = CostTracker(config)\n",
    "llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "\n",
    "# Read the real parquet file with embeddings\n",
    "print(\"Loading document dataset from parquet file...\")\n",
    "docs_df = pd.read_parquet(\"data/df_with_embeddings.parquet\")\n",
    "docs_df = docs_df.head(20)\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(f\"Available columns in parquet file: {docs_df.columns.tolist()}\")\n",
    "print(f\"Number of documents: {len(docs_df)}\")\n",
    "\n",
    "# Check for required columns\n",
    "if 'combined_text' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'combined_text' column found in the parquet file. Please check column names.\")\n",
    "if 'embedding' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'embedding' column found in the parquet file.\")\n",
    "\n",
    "# Keep only relevant columns to save memory and avoid duplicates\n",
    "docs_df = docs_df[['combined_text', 'embedding']].copy()\n",
    "docs_df = docs_df.rename(columns={'combined_text': 'Text'})\n",
    "\n",
    "# Create TOC_Number based on index\n",
    "docs_df['TOC_Number'] = [f\"{i//10+1}.{i%10+1}\" for i in range(len(docs_df))]\n",
    "\n",
    "# Verify embedding format (convert to list if needed)\n",
    "first_embedding = docs_df['embedding'].iloc[0]\n",
    "if isinstance(first_embedding, np.ndarray):\n",
    "    print(\"Converting embeddings from numpy arrays to lists...\")\n",
    "    docs_df['embedding'] = docs_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "print(f\"Sample text (first 200 chars): {docs_df['Text'].iloc[0][:200]}...\")\n",
    "print(f\"Embedding dimension: {len(docs_df['embedding'].iloc[0])}\")\n",
    "\n",
    "# Load golden source\n",
    "golden_df = pd.read_excel(\"golden_source.xlsx\")\n",
    "\n",
    "# Categorize documents\n",
    "print(\"\\nCategorizing documents...\")\n",
    "categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], llm, cost_tracker)\n",
    "categorized_df = categorizer.categorize_dataset(docs_df)\n",
    "categorized_df.to_parquet(\"document_dataset_categorized.parquet\", index=False)\n",
    "print(\"Categorized documents saved to parquet.\")\n",
    "\n",
    "# Initialize retriever with existing embeddings\n",
    "skip_embedding_generation = 'embedding' in categorized_df.columns\n",
    "retriever = DocumentRetriever(categorized_df, config, cost_tracker, skip_embedding_generation=skip_embedding_generation)\n",
    "\n",
    "# Run retrieval strategies for all questions\n",
    "all_results = {}\n",
    "print(\"\\nRunning retrieval strategies for all questions...\")\n",
    "for _, row in tqdm(golden_df.iterrows(), total=len(golden_df), desc=\"Evaluating Questions\"):\n",
    "    q_id, question = row['Question_ID'], row['Question_Text']\n",
    "    cost_tracker.cost_breakdown['query_categorization'] = 0\n",
    "    \n",
    "    all_results[q_id] = {\n",
    "        s_code: {'tocs': t, 'latency': l, 'cost': c}\n",
    "        for s_code, (t, l, c) in zip(\n",
    "            ['A', 'B', 'C', 'D'],\n",
    "            [retriever.retrieve(s, question, config[\"retrieval_k\"], q_id) for s in ['A', 'B', 'C', 'D']]\n",
    "        )\n",
    "    }\n",
    "    all_results[q_id]['categorization_cost'] = cost_tracker.cost_breakdown['query_categorization']\n",
    "    if hasattr(retriever, '_cat_cache'): retriever._cat_cache.clear()\n",
    "\n",
    "# Generate evaluation report\n",
    "evaluator = Evaluator(golden_df, config[\"retrieval_k\"])\n",
    "evaluator.generate_report(all_results, cost_tracker, retriever.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f65984c-9243-40ac-91b6-45f4d29aff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': {'A': {'tocs': ['1.8', '2.1', '1.2', '2.4', '2.6'],\n",
       "   'latency': 563.8754367828369,\n",
       "   'cost': 2.2e-07},\n",
       "  'B': {'tocs': ['1.1', '1.2', '1.3', '1.4', '1.5'],\n",
       "   'latency': 817.7018165588379,\n",
       "   'cost': 0},\n",
       "  'C': {'tocs': ['1.8', '2.1', '1.2', '2.4', '2.6'],\n",
       "   'latency': 540.198802947998,\n",
       "   'cost': 2.2e-07},\n",
       "  'D': {'tocs': ['1.6', '1.5', '1.4', '1.3', '1.2'],\n",
       "   'latency': 803.7383556365967,\n",
       "   'cost': 0.00154335},\n",
       "  'categorization_cost': 1.6499999999999999e-06},\n",
       " 'Q2': {'A': {'tocs': ['2.5', '2.1', '1.10', '1.6', '2.8'],\n",
       "   'latency': 441.8022632598877,\n",
       "   'cost': 2.2e-07},\n",
       "  'B': {'tocs': [], 'latency': 648.1449604034424, 'cost': 0},\n",
       "  'C': {'tocs': [], 'latency': 1.9271373748779297, 'cost': 2.2e-07},\n",
       "  'D': {'tocs': ['1.6', '1.5', '1.4', '1.3', '1.2'],\n",
       "   'latency': 705.0299644470215,\n",
       "   'cost': 0.00154335},\n",
       "  'categorization_cost': 1.6499999999999999e-06},\n",
       " 'Q3': {'A': {'tocs': ['1.6', '1.2', '2.5', '1.4', '1.9'],\n",
       "   'latency': 257.70044326782227,\n",
       "   'cost': 2.6e-07},\n",
       "  'B': {'tocs': ['1.1', '1.2', '1.3', '1.4', '1.5'],\n",
       "   'latency': 741.3234710693359,\n",
       "   'cost': 0},\n",
       "  'C': {'tocs': ['1.6', '1.2', '2.5', '1.4', '1.9'],\n",
       "   'latency': 805.1638603210449,\n",
       "   'cost': 2.6e-07},\n",
       "  'D': {'tocs': ['1.6', '1.5', '1.4', '1.3', '1.2'],\n",
       "   'latency': 701.1854648590088,\n",
       "   'cost': 0.00154365},\n",
       "  'categorization_cost': 3.7499999999999997e-06},\n",
       " 'Q4': {'A': {'tocs': ['1.8', '1.3', '1.2', '2.4', '2.5'],\n",
       "   'latency': 312.450647354126,\n",
       "   'cost': 2.0000000000000002e-07},\n",
       "  'B': {'tocs': ['1.1', '1.2', '1.3', '1.4', '1.5'],\n",
       "   'latency': 559.0732097625732,\n",
       "   'cost': 0},\n",
       "  'C': {'tocs': ['1.8', '1.3', '1.2', '2.4', '2.5'],\n",
       "   'latency': 246.92344665527344,\n",
       "   'cost': 2.0000000000000002e-07},\n",
       "  'D': {'tocs': ['1.6', '1.5', '1.4', '1.3', '1.2'],\n",
       "   'latency': 1882.779836654663,\n",
       "   'cost': 0.0015431999999999998},\n",
       "  'categorization_cost': 1.5e-06},\n",
       " 'Q5': {'A': {'tocs': ['2.5', '1.6', '2.1', '1.10', '1.2'],\n",
       "   'latency': 208.42432975769043,\n",
       "   'cost': 1.6e-07},\n",
       "  'B': {'tocs': [], 'latency': 604.2654514312744, 'cost': 0},\n",
       "  'C': {'tocs': [], 'latency': 2.5403499603271484, 'cost': 1.6e-07},\n",
       "  'D': {'tocs': ['1.6', '1.5', '1.4', '1.3', '1.2'],\n",
       "   'latency': 707.8120708465576,\n",
       "   'cost': 0.0015429},\n",
       "  'categorization_cost': 1.2e-06}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20780e9d-da0a-4307-bd62-e5b4d8cede82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
