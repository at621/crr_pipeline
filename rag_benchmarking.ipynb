{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16332433-4ad6-4147-bd38-8f88fd93ab2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0833958-08b2-4c99-9b61-ddb1538b1482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff287c3a-1979-43d6-81c2-659195f10fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document dataset from parquet file...\n",
      "Available columns in parquet file: ['Part', 'Title', 'Chapter', 'Section', 'Subsection', 'Part_Heading', 'Title_Heading', 'Chapter_Heading', 'Section_Heading', 'Subsection_Heading', 'Token_Count', 'Ends_With_Dot', 'Article_Number', 'Article_Heading', 'Text', 'Text_With_Pagebreaks', 'id', 'Category_1', 'Category_2', 'combined_text', 'embedding']\n",
      "Number of documents: 20\n",
      "Converting embeddings from numpy arrays to lists...\n",
      "Sample text (first 200 chars): Article_Heading: Article 1 - Scope...\n",
      "Embedding dimension: 1536\n",
      "Using existing embeddings from dataframe...\n",
      "\n",
      "Running retrieval strategies for all questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Questions: 100%|████████████████████████████████████████████████████████████| 50/50 [01:56<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import yaml\n",
    "\n",
    "from utils import DocumentCategorizer, CostTracker\n",
    "\n",
    "# Suppress LangChain tracer warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Error in LangChainTracer\")\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "\n",
    "# Disable LangChain tracing to avoid serialization errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"false\"\n",
    "\n",
    "# Load the configuration\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "# --- Helper Functions & Setup ---\n",
    "# def create_dummy_golden_file():\n",
    "#     \"\"\"Creates dummy golden source file for demonstration if it doesn't exist.\"\"\"\n",
    "#     golden_file = \"golden_source.xlsx\"\n",
    "#     if not os.path.exists(golden_file):\n",
    "#         print(f\"Creating dummy file: {golden_file}\")\n",
    "#         # Adjusted TOC numbers to match the format we'll generate (row_index//10+1).(row_index%10+1)\n",
    "#         pd.DataFrame({\n",
    "#             \"Question_ID\": [f\"Q{i}\" for i in range(1, 6)],\n",
    "#             \"Question_Text\": [\n",
    "#                 \"What are the IRB requirements for PD models?\",\n",
    "#                 \"How is LGD calculated under the SA approach?\",\n",
    "#                 \"What is the difference between PD and in-default LGD?\",\n",
    "#                 \"What are the key components of IRB models?\",\n",
    "#                 \"How does SA handle LGD estimation?\"\n",
    "#             ],\n",
    "#             \"TOC_1\": [\"1.1\", \"1.2\", \"1.3\", \"1.4\", \"1.5\"],\n",
    "#             \"TOC_2\": [\"2.1\", \"2.2\", \"2.3\", \"2.4\", \"2.5\"],\n",
    "#             \"TOC_3\": [\"3.1\", \"3.2\", \"3.3\", \"3.4\", \"3.5\"],\n",
    "#             \"TOC_4\": [\"4.1\", \"4.2\", \"4.3\", \"4.4\", \"4.5\"],\n",
    "#             \"TOC_5\": [\"5.1\", \"5.2\", \"5.3\", \"5.4\", \"5.5\"],\n",
    "#         }).to_excel(golden_file, index=False)\n",
    "#         print(f\"Note: Please update {golden_file} with actual TOC references from your documents.\")\n",
    "\n",
    "# --- System Components ---\n",
    "\n",
    "\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, df, config, cost_tracker, skip_embedding_generation=False):\n",
    "        self.df = df.copy()\n",
    "        self.config = config\n",
    "        self.cost_tracker = cost_tracker\n",
    "        self.llm_cache = {}\n",
    "        self.embedding_model = OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "        self.llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "        self.categorizer = DocumentCategorizer(config[\"Category_1\"], config[\"Category_2\"], self.llm, cost_tracker)\n",
    "        \n",
    "        if not skip_embedding_generation:\n",
    "            print(\"Generating document embeddings...\")\n",
    "            texts, total_tokens = self.df['Text'].tolist(), sum(len(t)//4 for t in self.df['Text'])\n",
    "            self.cost_tracker.add_cost(total_tokens, 'embedding', 'setup_embedding')\n",
    "            self.df['embedding'] = self.embedding_model.embed_documents(texts)\n",
    "        else:\n",
    "            print(\"Using existing embeddings from dataframe...\")\n",
    "            # Ensure embeddings are in the right format (list of floats)\n",
    "            if 'embedding' in self.df.columns:\n",
    "                self.df['embedding'] = self.df['embedding'].apply(lambda x: x if isinstance(x, list) else list(x))\n",
    "        \n",
    "    def _categorize_question(self, question, question_id):\n",
    "        if hasattr(self, '_cat_cache') and question_id in self._cat_cache: return self._cat_cache[question_id]\n",
    "        input_tokens, (cat1, cat2) = len(question)//4, self.categorizer.categorize_text(question)\n",
    "        output_tokens = len(cat1)//4 + len(cat2)//4\n",
    "        self.cost_tracker.add_cost(input_tokens, 'llm_input', 'query_categorization')\n",
    "        self.cost_tracker.add_cost(output_tokens, 'llm_output', 'query_categorization')\n",
    "        if not hasattr(self, '_cat_cache'): self._cat_cache = {}\n",
    "        self._cat_cache[question_id] = (cat1, cat2)\n",
    "        return cat1, cat2\n",
    "        \n",
    "    def retrieve(self, s_name, question, k, q_id):\n",
    "        start = time.time()\n",
    "        categories = None  # Initialize categories\n",
    "        \n",
    "        if s_name == 'A': \n",
    "            tocs, cost = self.retrieve_by_embedding(question, k)\n",
    "        elif s_name == 'B': \n",
    "            tocs, cost, categories = self.retrieve_by_category(question, k, q_id)\n",
    "        elif s_name == 'C': \n",
    "            tocs, cost, categories = self.retrieve_hybrid(question, k, q_id)\n",
    "        elif s_name == 'D': \n",
    "            tocs, cost = self.retrieve_full_context(question, k)\n",
    "        else: \n",
    "            raise ValueError(f\"Unknown strategy: {s_name}\")\n",
    "            \n",
    "        return tocs, (time.time() - start) * 1000, cost, categories\n",
    "        \n",
    "    def retrieve_by_embedding(self, question, k):\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        sims = cosine_similarity([q_emb], np.array(self.df['embedding'].tolist()))[0]\n",
    "        return self.df.iloc[np.argsort(sims)[::-1][:k]]['id'].tolist(), cost\n",
    "        \n",
    "    def retrieve_by_category(self, question, k, q_id):\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        df = self.df[(self.df['Category_1'] == cat1) & (self.df['Category_2'] == cat2)]\n",
    "        if df.empty: df = self.df[self.df['Category_1'] == cat1]\n",
    "        return df['id'].head(k).tolist(), 0, [cat1, cat2]\n",
    "        \n",
    "    def retrieve_hybrid(self, question, k, q_id):\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        # --- THIS IS THE CORRECTED LINE ---\n",
    "        df = pd.concat([\n",
    "            self.df[(self.df['Category_1']==cat1)&(self.df['Category_2']==cat2)], \n",
    "            self.df[self.df['Category_1']==cat1]\n",
    "        ]).drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "        # --- END OF CORRECTION ---\n",
    "        if df.empty: return [], cost, [cat1, cat2]\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        df['sim'] = cosine_similarity([q_emb], np.array(df['embedding'].tolist()))[0]\n",
    "        return df.sort_values('sim', ascending=False).head(k)['id'].tolist(), cost, [cat1, cat2]\n",
    "        \n",
    "    def retrieve_full_context(self, question, k):\n",
    "        if self.config['use_cache'] and question in self.llm_cache: return self.llm_cache[question][0], 0\n",
    "        context_str = \"\\n\".join([f\"TOC {row['id']}: {row['Text']}\" for _, row in self.df.iterrows()])\n",
    "        prompt = f\"\"\"Given the document context below, identify the TOP {k} `TOC_Number`s most relevant to the user's question. Return only a comma-separated list of TOC numbers (e.g., 1.1, 2.3, 3.2).\n",
    "CONTEXT:\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "QUESTION: \"{question}\"\n",
    "Relevant TOC_Numbers:\n",
    "\"\"\"\n",
    "        res = self.llm.invoke(prompt).content.strip()\n",
    "        tocs = [t.strip() for t in res.split(',')]\n",
    "        cost = self.cost_tracker.add_cost(len(prompt)//4, 'llm_input', 'query_llm_context') + \\\n",
    "               self.cost_tracker.add_cost(len(res)//4, 'llm_output', 'query_llm_context')\n",
    "        if self.config['use_cache']: self.llm_cache[question] = (tocs, cost)\n",
    "        return tocs, cost\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, golden_source_df, k):\n",
    "        self.k = k\n",
    "        self.golden_df = golden_source_df\n",
    "        toc_cols = [f'TOC_{i}' for i in range(1, 6)]\n",
    "        self.golden_map = {row['Question_ID']: set(row[toc_cols].astype(str).values) for _, row in self.golden_df.iterrows()}\n",
    "            \n",
    "    def evaluate_results(self, results_df):\n",
    "        \"\"\"Evaluate results from the DataFrame\"\"\"\n",
    "        # Add evaluation metrics\n",
    "        results_df['golden_tocs'] = results_df['question_id'].map(\n",
    "            lambda q_id: list(self.golden_map.get(q_id, set()))\n",
    "        )\n",
    "        results_df['matches'] = results_df.apply(\n",
    "            lambda row: len(set(row['retrieved_tocs']).intersection(set(row['golden_tocs']))), axis=1\n",
    "        )\n",
    "        results_df['precision'] = results_df['matches'] / self.k if self.k > 0 else 0\n",
    "        return results_df\n",
    "\n",
    "\n",
    "# --- Jupyter Notebook Execution Code ---\n",
    "# Create dummy golden file if needed\n",
    "# create_dummy_golden_file()\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set. Please create a .env file or set it in your notebook.\")\n",
    "\n",
    "# Initialize components\n",
    "cost_tracker = CostTracker(config)\n",
    "llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "\n",
    "# Read the real parquet file with embeddings\n",
    "print(\"Loading document dataset from parquet file...\")\n",
    "docs_df = pd.read_parquet(\"data/df_with_embeddings.parquet\")\n",
    "docs_df = docs_df.head(20)\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(f\"Available columns in parquet file: {docs_df.columns.tolist()}\")\n",
    "print(f\"Number of documents: {len(docs_df)}\")\n",
    "\n",
    "# Check for required columns\n",
    "if 'combined_text' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'combined_text' column found in the parquet file. Please check column names.\")\n",
    "if 'embedding' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'embedding' column found in the parquet file.\")\n",
    "\n",
    "# Keep only relevant columns to save memory and avoid duplicates\n",
    "docs_df = docs_df[['combined_text', 'embedding', 'Category_1', 'Category_2']].copy()\n",
    "docs_df = docs_df.rename(columns={'combined_text': 'Text'})\n",
    "\n",
    "# Create TOC_Number based on index\n",
    "docs_df['id'] = [f\"{i//10+1}.{i%10+1}\" for i in range(len(docs_df))]\n",
    "\n",
    "# Verify embedding format (convert to list if needed)\n",
    "first_embedding = docs_df['embedding'].iloc[0]\n",
    "if isinstance(first_embedding, np.ndarray):\n",
    "    print(\"Converting embeddings from numpy arrays to lists...\")\n",
    "    docs_df['embedding'] = docs_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "print(f\"Sample text (first 200 chars): {docs_df['Text'].iloc[0][:200]}...\")\n",
    "print(f\"Embedding dimension: {len(docs_df['embedding'].iloc[0])}\")\n",
    "\n",
    "# Load golden source\n",
    "golden_df = pd.read_excel(\"questions_benchmarking.xlsx\")\n",
    "\n",
    "# # Categorize documents\n",
    "# print(\"\\nCategorizing documents...\")\n",
    "# categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], llm, cost_tracker)\n",
    "# categorized_df = categorizer.categorize_dataset(docs_df)\n",
    "# categorized_df.to_parquet(\"document_dataset_categorized.parquet\", index=False)\n",
    "# print(\"Categorized documents saved to parquet.\")\n",
    "\n",
    "# Initialize retriever with existing embeddings\n",
    "skip_embedding_generation = 'embedding' in docs_df.columns\n",
    "retriever = DocumentRetriever(docs_df, config, cost_tracker, skip_embedding_generation=skip_embedding_generation)\n",
    "\n",
    "# Run retrieval strategies for all questions - SIMPLIFIED DATA STRUCTURE\n",
    "all_results = []  # List of dictionaries instead of nested dict\n",
    "print(\"\\nRunning retrieval strategies for all questions...\")\n",
    "\n",
    "for _, row in tqdm(golden_df.iterrows(), total=len(golden_df), desc=\"Evaluating Questions\"):\n",
    "    q_id, question = row['Question_ID'], row['Question_Text']\n",
    "    \n",
    "    # Reset categorization cost tracking for this question\n",
    "    cost_tracker.cost_breakdown['query_categorization'] = 0\n",
    "    q_cat_cost_before = cost_tracker.cost_breakdown['query_categorization']\n",
    "    \n",
    "    # Run each strategy and collect results\n",
    "    for strategy_code in ['A', 'B', 'C', 'D']:\n",
    "        tocs, latency, cost, categories = retriever.retrieve(strategy_code, question, config[\"retrieval_k\"], q_id)\n",
    "        \n",
    "        # Calculate total cost including categorization if applicable\n",
    "        if strategy_code in ['B', 'C']:\n",
    "            total_cost = cost + (cost_tracker.cost_breakdown['query_categorization'] - q_cat_cost_before)\n",
    "        else:\n",
    "            total_cost = cost\n",
    "            \n",
    "        # Append flat dictionary for this question-strategy combination\n",
    "        all_results.append({\n",
    "            'question_id': q_id,\n",
    "            'question_text': question,\n",
    "            'strategy': strategy_code,\n",
    "            'retrieved_tocs': tocs,\n",
    "            'latency': latency,\n",
    "            'cost': cost,\n",
    "            'categorization_cost': cost_tracker.cost_breakdown['query_categorization'] - q_cat_cost_before if strategy_code in ['B', 'C'] else 0,\n",
    "            'total_cost': total_cost,\n",
    "            'categories': categories  # Will be None for strategies A and D\n",
    "        })\n",
    "    \n",
    "    # Clear categorization cache after each question\n",
    "    if hasattr(retriever, '_cat_cache'): \n",
    "        retriever._cat_cache.clear()\n",
    "\n",
    "# Convert to DataFrame with one-liner\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Generate evaluation report\n",
    "evaluator = Evaluator(golden_df, config[\"retrieval_k\"])\n",
    "results_df = evaluator.evaluate_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4345c3-455d-4720-bc07-97e74b86c64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>strategy</th>\n",
       "      <th>retrieved_tocs</th>\n",
       "      <th>latency</th>\n",
       "      <th>cost</th>\n",
       "      <th>categorization_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>categories</th>\n",
       "      <th>golden_tocs</th>\n",
       "      <th>matches</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the definition of a 'rating system' fo...</td>\n",
       "      <td>A</td>\n",
       "      <td>[1.8, 1.6, 1.5, 2.5, 2.1]</td>\n",
       "      <td>392.942667</td>\n",
       "      <td>4.000000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the definition of a 'rating system' fo...</td>\n",
       "      <td>B</td>\n",
       "      <td>[]</td>\n",
       "      <td>1117.980003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.200000e-06</td>\n",
       "      <td>[IRB Approach Requirements, Other]</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the definition of a 'rating system' fo...</td>\n",
       "      <td>C</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.977682</td>\n",
       "      <td>4.000000e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.600000e-06</td>\n",
       "      <td>[IRB Approach Requirements, Other]</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the definition of a 'rating system' fo...</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.5, 1.6, 1.7, 2.1, 2.3]</td>\n",
       "      <td>900.285006</td>\n",
       "      <td>1.544700e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.544700e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the minimum asset threshold for a fina...</td>\n",
       "      <td>A</td>\n",
       "      <td>[2.9, 1.8, 1.2, 2.5, 2.4]</td>\n",
       "      <td>403.013945</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.600000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>49</td>\n",
       "      <td>What is the required level of independence for...</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.5, 1.6, 1.4, 2.1, 2.3]</td>\n",
       "      <td>908.309460</td>\n",
       "      <td>1.545900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545900e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>50</td>\n",
       "      <td>How frequently must an institution's internal ...</td>\n",
       "      <td>A</td>\n",
       "      <td>[1.8, 2.1, 2.6, 2.10, 2.4]</td>\n",
       "      <td>265.009403</td>\n",
       "      <td>5.200000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.200000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>50</td>\n",
       "      <td>How frequently must an institution's internal ...</td>\n",
       "      <td>B</td>\n",
       "      <td>[]</td>\n",
       "      <td>667.815208</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>9.900000e-06</td>\n",
       "      <td>[Operational Risk, IRB Approach Requirements]</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>50</td>\n",
       "      <td>How frequently must an institution's internal ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[]</td>\n",
       "      <td>2.616167</td>\n",
       "      <td>5.200000e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.042000e-05</td>\n",
       "      <td>[Operational Risk, IRB Approach Requirements]</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>50</td>\n",
       "      <td>How frequently must an institution's internal ...</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.1, 1.2, 1.3, 1.4, 1.5]</td>\n",
       "      <td>647.231579</td>\n",
       "      <td>1.545600e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545600e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[5, 7, 4, 3, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     question_id                                      question_text strategy  \\\n",
       "0              1  What is the definition of a 'rating system' fo...        A   \n",
       "1              1  What is the definition of a 'rating system' fo...        B   \n",
       "2              1  What is the definition of a 'rating system' fo...        C   \n",
       "3              1  What is the definition of a 'rating system' fo...        D   \n",
       "4              2  What is the minimum asset threshold for a fina...        A   \n",
       "..           ...                                                ...      ...   \n",
       "195           49  What is the required level of independence for...        D   \n",
       "196           50  How frequently must an institution's internal ...        A   \n",
       "197           50  How frequently must an institution's internal ...        B   \n",
       "198           50  How frequently must an institution's internal ...        C   \n",
       "199           50  How frequently must an institution's internal ...        D   \n",
       "\n",
       "                 retrieved_tocs      latency          cost  \\\n",
       "0     [1.8, 1.6, 1.5, 2.5, 2.1]   392.942667  4.000000e-07   \n",
       "1                            []  1117.980003  0.000000e+00   \n",
       "2                            []     1.977682  4.000000e-07   \n",
       "3     [1.5, 1.6, 1.7, 2.1, 2.3]   900.285006  1.544700e-03   \n",
       "4     [2.9, 1.8, 1.2, 2.5, 2.4]   403.013945  8.600000e-07   \n",
       "..                          ...          ...           ...   \n",
       "195   [1.5, 1.6, 1.4, 2.1, 2.3]   908.309460  1.545900e-03   \n",
       "196  [1.8, 2.1, 2.6, 2.10, 2.4]   265.009403  5.200000e-07   \n",
       "197                          []   667.815208  0.000000e+00   \n",
       "198                          []     2.616167  5.200000e-07   \n",
       "199   [1.1, 1.2, 1.3, 1.4, 1.5]   647.231579  1.545600e-03   \n",
       "\n",
       "     categorization_cost    total_cost  \\\n",
       "0               0.000000  4.000000e-07   \n",
       "1               0.000007  7.200000e-06   \n",
       "2               0.000007  7.600000e-06   \n",
       "3               0.000000  1.544700e-03   \n",
       "4               0.000000  8.600000e-07   \n",
       "..                   ...           ...   \n",
       "195             0.000000  1.545900e-03   \n",
       "196             0.000000  5.200000e-07   \n",
       "197             0.000010  9.900000e-06   \n",
       "198             0.000010  1.042000e-05   \n",
       "199             0.000000  1.545600e-03   \n",
       "\n",
       "                                        categories      golden_tocs  matches  \\\n",
       "0                                             None  [5, 7, 4, 3, 6]        0   \n",
       "1               [IRB Approach Requirements, Other]  [5, 7, 4, 3, 6]        0   \n",
       "2               [IRB Approach Requirements, Other]  [5, 7, 4, 3, 6]        0   \n",
       "3                                             None  [5, 7, 4, 3, 6]        0   \n",
       "4                                             None  [5, 7, 4, 3, 6]        0   \n",
       "..                                             ...              ...      ...   \n",
       "195                                           None  [5, 7, 4, 3, 6]        0   \n",
       "196                                           None  [5, 7, 4, 3, 6]        0   \n",
       "197  [Operational Risk, IRB Approach Requirements]  [5, 7, 4, 3, 6]        0   \n",
       "198  [Operational Risk, IRB Approach Requirements]  [5, 7, 4, 3, 6]        0   \n",
       "199                                           None  [5, 7, 4, 3, 6]        0   \n",
       "\n",
       "     precision  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "..         ...  \n",
       "195        0.0  \n",
       "196        0.0  \n",
       "197        0.0  \n",
       "198        0.0  \n",
       "199        0.0  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec381e-5696-4315-8571-28f291255844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
