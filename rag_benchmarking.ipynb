{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16332433-4ad6-4147-bd38-8f88fd93ab2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0833958-08b2-4c99-9b61-ddb1538b1482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff287c3a-1979-43d6-81c2-659195f10fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document dataset from parquet file...\n",
      "Available columns in parquet file: ['Part', 'Title', 'Chapter', 'Section', 'Subsection', 'Part_Heading', 'Title_Heading', 'Chapter_Heading', 'Section_Heading', 'Subsection_Heading', 'Token_Count', 'Ends_With_Dot', 'Article_Number', 'Article_Heading', 'Text', 'Text_With_Pagebreaks', 'combined_text', 'embedding']\n",
      "Number of documents: 20\n",
      "Converting embeddings from numpy arrays to lists...\n",
      "Sample text (first 200 chars): Article_Heading: Article 1 - Scope...\n",
      "Embedding dimension: 1536\n",
      "\n",
      "Categorizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorizing Documents: 100%|██████████████████████████████████████████████████████████| 20/20 [00:12<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorized documents saved to parquet.\n",
      "Using existing embeddings from dataframe...\n",
      "\n",
      "Running retrieval strategies for all questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Questions: 100%|██████████████████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import yaml\n",
    "\n",
    "from utils import DocumentCategorizer, CostTracker\n",
    "\n",
    "# Suppress LangChain tracer warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Error in LangChainTracer\")\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "\n",
    "# Disable LangChain tracing to avoid serialization errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"false\"\n",
    "\n",
    "# Load the configuration\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "# --- Helper Functions & Setup ---\n",
    "def create_dummy_golden_file():\n",
    "    \"\"\"Creates dummy golden source file for demonstration if it doesn't exist.\"\"\"\n",
    "    golden_file = \"golden_source.xlsx\"\n",
    "    if not os.path.exists(golden_file):\n",
    "        print(f\"Creating dummy file: {golden_file}\")\n",
    "        # Adjusted TOC numbers to match the format we'll generate (row_index//10+1).(row_index%10+1)\n",
    "        pd.DataFrame({\n",
    "            \"Question_ID\": [f\"Q{i}\" for i in range(1, 6)],\n",
    "            \"Question_Text\": [\n",
    "                \"What are the IRB requirements for PD models?\",\n",
    "                \"How is LGD calculated under the SA approach?\",\n",
    "                \"What is the difference between PD and in-default LGD?\",\n",
    "                \"What are the key components of IRB models?\",\n",
    "                \"How does SA handle LGD estimation?\"\n",
    "            ],\n",
    "            \"TOC_1\": [\"1.1\", \"1.2\", \"1.3\", \"1.4\", \"1.5\"],\n",
    "            \"TOC_2\": [\"2.1\", \"2.2\", \"2.3\", \"2.4\", \"2.5\"],\n",
    "            \"TOC_3\": [\"3.1\", \"3.2\", \"3.3\", \"3.4\", \"3.5\"],\n",
    "            \"TOC_4\": [\"4.1\", \"4.2\", \"4.3\", \"4.4\", \"4.5\"],\n",
    "            \"TOC_5\": [\"5.1\", \"5.2\", \"5.3\", \"5.4\", \"5.5\"],\n",
    "        }).to_excel(golden_file, index=False)\n",
    "        print(f\"Note: Please update {golden_file} with actual TOC references from your documents.\")\n",
    "\n",
    "# --- System Components ---\n",
    "\n",
    "\n",
    "\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, df, config, cost_tracker, skip_embedding_generation=False):\n",
    "        self.df, self.config, self.cost_tracker, self.llm_cache = df.copy(), config, cost_tracker, {}\n",
    "        self.embedding_model = OpenAIEmbeddings(model=config[\"embedding_model\"])\n",
    "        self.llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "        self.categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], self.llm, cost_tracker)\n",
    "        \n",
    "        if not skip_embedding_generation:\n",
    "            print(\"Generating document embeddings...\")\n",
    "            texts, total_tokens = self.df['Text'].tolist(), sum(len(t)//4 for t in self.df['Text'])\n",
    "            self.cost_tracker.add_cost(total_tokens, 'embedding', 'setup_embedding')\n",
    "            self.df['embedding'] = self.embedding_model.embed_documents(texts)\n",
    "        else:\n",
    "            print(\"Using existing embeddings from dataframe...\")\n",
    "            # Ensure embeddings are in the right format (list of floats)\n",
    "            if 'embedding' in self.df.columns:\n",
    "                self.df['embedding'] = self.df['embedding'].apply(lambda x: x if isinstance(x, list) else list(x))\n",
    "        \n",
    "    def _categorize_question(self, question, question_id):\n",
    "        if hasattr(self, '_cat_cache') and question_id in self._cat_cache: return self._cat_cache[question_id]\n",
    "        input_tokens, (cat1, cat2) = len(question)//4, self.categorizer.categorize_text(question)\n",
    "        output_tokens = len(cat1)//4 + len(cat2)//4\n",
    "        self.cost_tracker.add_cost(input_tokens, 'llm_input', 'query_categorization')\n",
    "        self.cost_tracker.add_cost(output_tokens, 'llm_output', 'query_categorization')\n",
    "        if not hasattr(self, '_cat_cache'): self._cat_cache = {}\n",
    "        self._cat_cache[question_id] = (cat1, cat2)\n",
    "        return cat1, cat2\n",
    "        \n",
    "    def retrieve(self, s_name, question, k, q_id):\n",
    "        start = time.time()\n",
    "        categories = None  # Initialize categories\n",
    "        \n",
    "        if s_name == 'A': \n",
    "            tocs, cost = self.retrieve_by_embedding(question, k)\n",
    "        elif s_name == 'B': \n",
    "            tocs, cost, categories = self.retrieve_by_category(question, k, q_id)\n",
    "        elif s_name == 'C': \n",
    "            tocs, cost, categories = self.retrieve_hybrid(question, k, q_id)\n",
    "        elif s_name == 'D': \n",
    "            tocs, cost = self.retrieve_full_context(question, k)\n",
    "        else: \n",
    "            raise ValueError(f\"Unknown strategy: {s_name}\")\n",
    "            \n",
    "        return tocs, (time.time() - start) * 1000, cost, categories\n",
    "        \n",
    "    def retrieve_by_embedding(self, question, k):\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        sims = cosine_similarity([q_emb], np.array(self.df['embedding'].tolist()))[0]\n",
    "        return self.df.iloc[np.argsort(sims)[::-1][:k]]['TOC_Number'].tolist(), cost\n",
    "        \n",
    "    def retrieve_by_category(self, question, k, q_id):\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        df = self.df[(self.df['Category_1'] == cat1) & (self.df['Category_2'] == cat2)]\n",
    "        if df.empty: df = self.df[self.df['Category_1'] == cat1]\n",
    "        return df['TOC_Number'].head(k).tolist(), 0, [cat1, cat2]\n",
    "        \n",
    "    def retrieve_hybrid(self, question, k, q_id):\n",
    "        cost = self.cost_tracker.add_cost(len(question)//4, \"embedding\", \"query_embedding\")\n",
    "        cat1, cat2 = self._categorize_question(question, q_id)\n",
    "        # --- THIS IS THE CORRECTED LINE ---\n",
    "        df = pd.concat([\n",
    "            self.df[(self.df['Category_1']==cat1)&(self.df['Category_2']==cat2)], \n",
    "            self.df[self.df['Category_1']==cat1]\n",
    "        ]).drop_duplicates(subset=['TOC_Number']).reset_index(drop=True)\n",
    "        # --- END OF CORRECTION ---\n",
    "        if df.empty: return [], cost, [cat1, cat2]\n",
    "        q_emb = self.embedding_model.embed_query(question)\n",
    "        df['sim'] = cosine_similarity([q_emb], np.array(df['embedding'].tolist()))[0]\n",
    "        return df.sort_values('sim', ascending=False).head(k)['TOC_Number'].tolist(), cost, [cat1, cat2]\n",
    "        \n",
    "    def retrieve_full_context(self, question, k):\n",
    "        if self.config['use_cache'] and question in self.llm_cache: return self.llm_cache[question][0], 0\n",
    "        context_str = \"\\n\".join([f\"TOC {row['TOC_Number']}: {row['Text']}\" for _, row in self.df.iterrows()])\n",
    "        prompt = f\"\"\"Given the document context below, identify the TOP {k} `TOC_Number`s most relevant to the user's question. Return only a comma-separated list of TOC numbers (e.g., 1.1, 2.3, 3.2).\n",
    "CONTEXT:\n",
    "---\n",
    "{context_str}\n",
    "---\n",
    "QUESTION: \"{question}\"\n",
    "Relevant TOC_Numbers:\n",
    "\"\"\"\n",
    "        res = self.llm.invoke(prompt).content.strip()\n",
    "        tocs = [t.strip() for t in res.split(',')]\n",
    "        cost = self.cost_tracker.add_cost(len(prompt)//4, 'llm_input', 'query_llm_context') + \\\n",
    "               self.cost_tracker.add_cost(len(res)//4, 'llm_output', 'query_llm_context')\n",
    "        if self.config['use_cache']: self.llm_cache[question] = (tocs, cost)\n",
    "        return tocs, cost\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, golden_source_df, k):\n",
    "        self.k = k\n",
    "        self.golden_df = golden_source_df\n",
    "        toc_cols = [f'TOC_{i}' for i in range(1, 6)]\n",
    "        self.golden_map = {row['Question_ID']: set(row[toc_cols].astype(str).values) for _, row in self.golden_df.iterrows()}\n",
    "            \n",
    "    def evaluate_results(self, results_df):\n",
    "        \"\"\"Evaluate results from the DataFrame\"\"\"\n",
    "        # Add evaluation metrics\n",
    "        results_df['golden_tocs'] = results_df['question_id'].map(\n",
    "            lambda q_id: list(self.golden_map.get(q_id, set()))\n",
    "        )\n",
    "        results_df['matches'] = results_df.apply(\n",
    "            lambda row: len(set(row['retrieved_tocs']).intersection(set(row['golden_tocs']))), axis=1\n",
    "        )\n",
    "        results_df['precision'] = results_df['matches'] / self.k if self.k > 0 else 0\n",
    "        return results_df\n",
    "\n",
    "\n",
    "# --- Jupyter Notebook Execution Code ---\n",
    "# Create dummy golden file if needed\n",
    "create_dummy_golden_file()\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set. Please create a .env file or set it in your notebook.\")\n",
    "\n",
    "# Initialize components\n",
    "cost_tracker = CostTracker(config)\n",
    "llm = ChatOpenAI(model=config[\"llm_model\"], temperature=0)\n",
    "\n",
    "# Read the real parquet file with embeddings\n",
    "print(\"Loading document dataset from parquet file...\")\n",
    "docs_df = pd.read_parquet(\"data/df_with_embeddings.parquet\")\n",
    "docs_df = docs_df.head(20)\n",
    "\n",
    "# Print available columns for debugging\n",
    "print(f\"Available columns in parquet file: {docs_df.columns.tolist()}\")\n",
    "print(f\"Number of documents: {len(docs_df)}\")\n",
    "\n",
    "# Check for required columns\n",
    "if 'combined_text' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'combined_text' column found in the parquet file. Please check column names.\")\n",
    "if 'embedding' not in docs_df.columns:\n",
    "    raise ValueError(\"No 'embedding' column found in the parquet file.\")\n",
    "\n",
    "# Keep only relevant columns to save memory and avoid duplicates\n",
    "docs_df = docs_df[['combined_text', 'embedding']].copy()\n",
    "docs_df = docs_df.rename(columns={'combined_text': 'Text'})\n",
    "\n",
    "# Create TOC_Number based on index\n",
    "docs_df['TOC_Number'] = [f\"{i//10+1}.{i%10+1}\" for i in range(len(docs_df))]\n",
    "\n",
    "# Verify embedding format (convert to list if needed)\n",
    "first_embedding = docs_df['embedding'].iloc[0]\n",
    "if isinstance(first_embedding, np.ndarray):\n",
    "    print(\"Converting embeddings from numpy arrays to lists...\")\n",
    "    docs_df['embedding'] = docs_df['embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "print(f\"Sample text (first 200 chars): {docs_df['Text'].iloc[0][:200]}...\")\n",
    "print(f\"Embedding dimension: {len(docs_df['embedding'].iloc[0])}\")\n",
    "\n",
    "# Load golden source\n",
    "golden_df = pd.read_excel(\"golden_source.xlsx\")\n",
    "\n",
    "# Categorize documents\n",
    "print(\"\\nCategorizing documents...\")\n",
    "categorizer = DocumentCategorizer(config[\"categories_1\"], config[\"categories_2\"], llm, cost_tracker)\n",
    "categorized_df = categorizer.categorize_dataset(docs_df)\n",
    "categorized_df.to_parquet(\"document_dataset_categorized.parquet\", index=False)\n",
    "print(\"Categorized documents saved to parquet.\")\n",
    "\n",
    "# Initialize retriever with existing embeddings\n",
    "skip_embedding_generation = 'embedding' in categorized_df.columns\n",
    "retriever = DocumentRetriever(categorized_df, config, cost_tracker, skip_embedding_generation=skip_embedding_generation)\n",
    "\n",
    "# Run retrieval strategies for all questions - SIMPLIFIED DATA STRUCTURE\n",
    "all_results = []  # List of dictionaries instead of nested dict\n",
    "print(\"\\nRunning retrieval strategies for all questions...\")\n",
    "\n",
    "for _, row in tqdm(golden_df.iterrows(), total=len(golden_df), desc=\"Evaluating Questions\"):\n",
    "    q_id, question = row['Question_ID'], row['Question_Text']\n",
    "    \n",
    "    # Reset categorization cost tracking for this question\n",
    "    cost_tracker.cost_breakdown['query_categorization'] = 0\n",
    "    q_cat_cost_before = cost_tracker.cost_breakdown['query_categorization']\n",
    "    \n",
    "    # Run each strategy and collect results\n",
    "    for strategy_code in ['A', 'B', 'C', 'D']:\n",
    "        tocs, latency, cost, categories = retriever.retrieve(strategy_code, question, config[\"retrieval_k\"], q_id)\n",
    "        \n",
    "        # Calculate total cost including categorization if applicable\n",
    "        if strategy_code in ['B', 'C']:\n",
    "            total_cost = cost + (cost_tracker.cost_breakdown['query_categorization'] - q_cat_cost_before)\n",
    "        else:\n",
    "            total_cost = cost\n",
    "            \n",
    "        # Append flat dictionary for this question-strategy combination\n",
    "        all_results.append({\n",
    "            'question_id': q_id,\n",
    "            'question_text': question,\n",
    "            'strategy': strategy_code,\n",
    "            'retrieved_tocs': tocs,\n",
    "            'latency': latency,\n",
    "            'cost': cost,\n",
    "            'categorization_cost': cost_tracker.cost_breakdown['query_categorization'] - q_cat_cost_before if strategy_code in ['B', 'C'] else 0,\n",
    "            'total_cost': total_cost,\n",
    "            'categories': categories  # Will be None for strategies A and D\n",
    "        })\n",
    "    \n",
    "    # Clear categorization cache after each question\n",
    "    if hasattr(retriever, '_cat_cache'): \n",
    "        retriever._cat_cache.clear()\n",
    "\n",
    "# Convert to DataFrame with one-liner\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Generate evaluation report\n",
    "evaluator = Evaluator(golden_df, config[\"retrieval_k\"])\n",
    "results_df = evaluator.evaluate_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4345c3-455d-4720-bc07-97e74b86c64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>strategy</th>\n",
       "      <th>retrieved_tocs</th>\n",
       "      <th>latency</th>\n",
       "      <th>cost</th>\n",
       "      <th>categorization_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>categories</th>\n",
       "      <th>golden_tocs</th>\n",
       "      <th>matches</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>What are the IRB requirements for PD models?</td>\n",
       "      <td>A</td>\n",
       "      <td>[1.8, 2.1, 1.2, 2.4, 2.6]</td>\n",
       "      <td>435.259342</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 2.3, 2.2, 1.2, 1.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>What are the IRB requirements for PD models?</td>\n",
       "      <td>B</td>\n",
       "      <td>[1.1, 1.2, 1.3, 1.4, 1.5]</td>\n",
       "      <td>630.033493</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.650000e-06</td>\n",
       "      <td>[IRB, PD]</td>\n",
       "      <td>[4.1, 2.3, 2.2, 1.2, 1.1]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>What are the IRB requirements for PD models?</td>\n",
       "      <td>C</td>\n",
       "      <td>[1.8, 2.1, 1.2, 2.4, 2.6]</td>\n",
       "      <td>230.580807</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.870000e-06</td>\n",
       "      <td>[IRB, PD]</td>\n",
       "      <td>[4.1, 2.3, 2.2, 1.2, 1.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>What are the IRB requirements for PD models?</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.6, 1.5, 1.4, 1.3, 1.2]</td>\n",
       "      <td>700.692892</td>\n",
       "      <td>1.543350e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543350e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 2.3, 2.2, 1.2, 1.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2</td>\n",
       "      <td>How is LGD calculated under the SA approach?</td>\n",
       "      <td>A</td>\n",
       "      <td>[2.5, 2.1, 1.10, 1.6, 2.8]</td>\n",
       "      <td>249.431372</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 3.1, 3.2, 3.3, 1.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q2</td>\n",
       "      <td>How is LGD calculated under the SA approach?</td>\n",
       "      <td>B</td>\n",
       "      <td>[1.7]</td>\n",
       "      <td>533.830643</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.650000e-06</td>\n",
       "      <td>[SA, LGD]</td>\n",
       "      <td>[4.1, 3.1, 3.2, 3.3, 1.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q2</td>\n",
       "      <td>How is LGD calculated under the SA approach?</td>\n",
       "      <td>C</td>\n",
       "      <td>[1.7]</td>\n",
       "      <td>215.156078</td>\n",
       "      <td>2.200000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.870000e-06</td>\n",
       "      <td>[SA, LGD]</td>\n",
       "      <td>[4.1, 3.1, 3.2, 3.3, 1.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q2</td>\n",
       "      <td>How is LGD calculated under the SA approach?</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.6, 1.5, 1.4, 1.3, 1.2]</td>\n",
       "      <td>737.566710</td>\n",
       "      <td>1.543350e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543350e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 3.1, 3.2, 3.3, 1.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q3</td>\n",
       "      <td>What is the difference between PD and in-defau...</td>\n",
       "      <td>A</td>\n",
       "      <td>[1.6, 1.2, 2.5, 1.4, 1.9]</td>\n",
       "      <td>260.808706</td>\n",
       "      <td>2.600000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 1.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q3</td>\n",
       "      <td>What is the difference between PD and in-defau...</td>\n",
       "      <td>B</td>\n",
       "      <td>[1.1, 1.2, 1.3, 1.4, 1.5]</td>\n",
       "      <td>986.675024</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3.750000e-06</td>\n",
       "      <td>[IRB, In-default LGD]</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 1.1]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Q3</td>\n",
       "      <td>What is the difference between PD and in-defau...</td>\n",
       "      <td>C</td>\n",
       "      <td>[1.6, 1.2, 2.5, 1.4, 1.9]</td>\n",
       "      <td>217.557669</td>\n",
       "      <td>2.600000e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.010000e-06</td>\n",
       "      <td>[IRB, In-default LGD]</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 1.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Q3</td>\n",
       "      <td>What is the difference between PD and in-defau...</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.6, 1.5, 1.4, 1.3, 1.2]</td>\n",
       "      <td>809.699297</td>\n",
       "      <td>1.543650e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543650e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 1.1]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q4</td>\n",
       "      <td>What are the key components of IRB models?</td>\n",
       "      <td>A</td>\n",
       "      <td>[1.8, 1.3, 1.2, 2.4, 2.5]</td>\n",
       "      <td>182.675600</td>\n",
       "      <td>2.000000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 2.2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Q4</td>\n",
       "      <td>What are the key components of IRB models?</td>\n",
       "      <td>B</td>\n",
       "      <td>[1.1, 1.2, 1.3, 1.4, 1.5]</td>\n",
       "      <td>701.519728</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.500000e-06</td>\n",
       "      <td>[IRB, PD]</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 2.2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Q4</td>\n",
       "      <td>What are the key components of IRB models?</td>\n",
       "      <td>C</td>\n",
       "      <td>[1.8, 1.3, 1.2, 2.4, 2.5]</td>\n",
       "      <td>193.618774</td>\n",
       "      <td>2.000000e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.700000e-06</td>\n",
       "      <td>[IRB, PD]</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 2.2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Q4</td>\n",
       "      <td>What are the key components of IRB models?</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.6, 1.5, 1.4, 1.3, 1.2]</td>\n",
       "      <td>796.781540</td>\n",
       "      <td>1.543200e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.543200e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[1.3, 3.1, 1.2, 2.1, 2.2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q5</td>\n",
       "      <td>How does SA handle LGD estimation?</td>\n",
       "      <td>A</td>\n",
       "      <td>[2.5, 1.6, 2.1, 1.10, 1.2]</td>\n",
       "      <td>181.735039</td>\n",
       "      <td>1.600000e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000e-07</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 2.3, 1.2, 3.2, 2.1]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Q5</td>\n",
       "      <td>How does SA handle LGD estimation?</td>\n",
       "      <td>B</td>\n",
       "      <td>[1.7]</td>\n",
       "      <td>598.205328</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.200000e-06</td>\n",
       "      <td>[SA, LGD]</td>\n",
       "      <td>[4.1, 2.3, 1.2, 3.2, 2.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Q5</td>\n",
       "      <td>How does SA handle LGD estimation?</td>\n",
       "      <td>C</td>\n",
       "      <td>[1.7]</td>\n",
       "      <td>176.778316</td>\n",
       "      <td>1.600000e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.360000e-06</td>\n",
       "      <td>[SA, LGD]</td>\n",
       "      <td>[4.1, 2.3, 1.2, 3.2, 2.1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Q5</td>\n",
       "      <td>How does SA handle LGD estimation?</td>\n",
       "      <td>D</td>\n",
       "      <td>[1.6, 1.5, 1.4, 1.3, 1.2]</td>\n",
       "      <td>1061.380863</td>\n",
       "      <td>1.542900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.542900e-03</td>\n",
       "      <td>None</td>\n",
       "      <td>[4.1, 2.3, 1.2, 3.2, 2.1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                      question_text strategy  \\\n",
       "0           Q1       What are the IRB requirements for PD models?        A   \n",
       "1           Q1       What are the IRB requirements for PD models?        B   \n",
       "2           Q1       What are the IRB requirements for PD models?        C   \n",
       "3           Q1       What are the IRB requirements for PD models?        D   \n",
       "4           Q2       How is LGD calculated under the SA approach?        A   \n",
       "5           Q2       How is LGD calculated under the SA approach?        B   \n",
       "6           Q2       How is LGD calculated under the SA approach?        C   \n",
       "7           Q2       How is LGD calculated under the SA approach?        D   \n",
       "8           Q3  What is the difference between PD and in-defau...        A   \n",
       "9           Q3  What is the difference between PD and in-defau...        B   \n",
       "10          Q3  What is the difference between PD and in-defau...        C   \n",
       "11          Q3  What is the difference between PD and in-defau...        D   \n",
       "12          Q4         What are the key components of IRB models?        A   \n",
       "13          Q4         What are the key components of IRB models?        B   \n",
       "14          Q4         What are the key components of IRB models?        C   \n",
       "15          Q4         What are the key components of IRB models?        D   \n",
       "16          Q5                 How does SA handle LGD estimation?        A   \n",
       "17          Q5                 How does SA handle LGD estimation?        B   \n",
       "18          Q5                 How does SA handle LGD estimation?        C   \n",
       "19          Q5                 How does SA handle LGD estimation?        D   \n",
       "\n",
       "                retrieved_tocs      latency          cost  \\\n",
       "0    [1.8, 2.1, 1.2, 2.4, 2.6]   435.259342  2.200000e-07   \n",
       "1    [1.1, 1.2, 1.3, 1.4, 1.5]   630.033493  0.000000e+00   \n",
       "2    [1.8, 2.1, 1.2, 2.4, 2.6]   230.580807  2.200000e-07   \n",
       "3    [1.6, 1.5, 1.4, 1.3, 1.2]   700.692892  1.543350e-03   \n",
       "4   [2.5, 2.1, 1.10, 1.6, 2.8]   249.431372  2.200000e-07   \n",
       "5                        [1.7]   533.830643  0.000000e+00   \n",
       "6                        [1.7]   215.156078  2.200000e-07   \n",
       "7    [1.6, 1.5, 1.4, 1.3, 1.2]   737.566710  1.543350e-03   \n",
       "8    [1.6, 1.2, 2.5, 1.4, 1.9]   260.808706  2.600000e-07   \n",
       "9    [1.1, 1.2, 1.3, 1.4, 1.5]   986.675024  0.000000e+00   \n",
       "10   [1.6, 1.2, 2.5, 1.4, 1.9]   217.557669  2.600000e-07   \n",
       "11   [1.6, 1.5, 1.4, 1.3, 1.2]   809.699297  1.543650e-03   \n",
       "12   [1.8, 1.3, 1.2, 2.4, 2.5]   182.675600  2.000000e-07   \n",
       "13   [1.1, 1.2, 1.3, 1.4, 1.5]   701.519728  0.000000e+00   \n",
       "14   [1.8, 1.3, 1.2, 2.4, 2.5]   193.618774  2.000000e-07   \n",
       "15   [1.6, 1.5, 1.4, 1.3, 1.2]   796.781540  1.543200e-03   \n",
       "16  [2.5, 1.6, 2.1, 1.10, 1.2]   181.735039  1.600000e-07   \n",
       "17                       [1.7]   598.205328  0.000000e+00   \n",
       "18                       [1.7]   176.778316  1.600000e-07   \n",
       "19   [1.6, 1.5, 1.4, 1.3, 1.2]  1061.380863  1.542900e-03   \n",
       "\n",
       "    categorization_cost    total_cost             categories  \\\n",
       "0              0.000000  2.200000e-07                   None   \n",
       "1              0.000002  1.650000e-06              [IRB, PD]   \n",
       "2              0.000002  1.870000e-06              [IRB, PD]   \n",
       "3              0.000000  1.543350e-03                   None   \n",
       "4              0.000000  2.200000e-07                   None   \n",
       "5              0.000002  1.650000e-06              [SA, LGD]   \n",
       "6              0.000002  1.870000e-06              [SA, LGD]   \n",
       "7              0.000000  1.543350e-03                   None   \n",
       "8              0.000000  2.600000e-07                   None   \n",
       "9              0.000004  3.750000e-06  [IRB, In-default LGD]   \n",
       "10             0.000004  4.010000e-06  [IRB, In-default LGD]   \n",
       "11             0.000000  1.543650e-03                   None   \n",
       "12             0.000000  2.000000e-07                   None   \n",
       "13             0.000002  1.500000e-06              [IRB, PD]   \n",
       "14             0.000002  1.700000e-06              [IRB, PD]   \n",
       "15             0.000000  1.543200e-03                   None   \n",
       "16             0.000000  1.600000e-07                   None   \n",
       "17             0.000001  1.200000e-06              [SA, LGD]   \n",
       "18             0.000001  1.360000e-06              [SA, LGD]   \n",
       "19             0.000000  1.542900e-03                   None   \n",
       "\n",
       "                  golden_tocs  matches  precision  \n",
       "0   [4.1, 2.3, 2.2, 1.2, 1.1]        1        0.2  \n",
       "1   [4.1, 2.3, 2.2, 1.2, 1.1]        2        0.4  \n",
       "2   [4.1, 2.3, 2.2, 1.2, 1.1]        1        0.2  \n",
       "3   [4.1, 2.3, 2.2, 1.2, 1.1]        1        0.2  \n",
       "4   [4.1, 3.1, 3.2, 3.3, 1.1]        0        0.0  \n",
       "5   [4.1, 3.1, 3.2, 3.3, 1.1]        0        0.0  \n",
       "6   [4.1, 3.1, 3.2, 3.3, 1.1]        0        0.0  \n",
       "7   [4.1, 3.1, 3.2, 3.3, 1.1]        0        0.0  \n",
       "8   [1.3, 3.1, 1.2, 2.1, 1.1]        1        0.2  \n",
       "9   [1.3, 3.1, 1.2, 2.1, 1.1]        3        0.6  \n",
       "10  [1.3, 3.1, 1.2, 2.1, 1.1]        1        0.2  \n",
       "11  [1.3, 3.1, 1.2, 2.1, 1.1]        2        0.4  \n",
       "12  [1.3, 3.1, 1.2, 2.1, 2.2]        2        0.4  \n",
       "13  [1.3, 3.1, 1.2, 2.1, 2.2]        2        0.4  \n",
       "14  [1.3, 3.1, 1.2, 2.1, 2.2]        2        0.4  \n",
       "15  [1.3, 3.1, 1.2, 2.1, 2.2]        2        0.4  \n",
       "16  [4.1, 2.3, 1.2, 3.2, 2.1]        2        0.4  \n",
       "17  [4.1, 2.3, 1.2, 3.2, 2.1]        0        0.0  \n",
       "18  [4.1, 2.3, 1.2, 3.2, 2.1]        0        0.0  \n",
       "19  [4.1, 2.3, 1.2, 3.2, 2.1]        1        0.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec381e-5696-4315-8571-28f291255844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
